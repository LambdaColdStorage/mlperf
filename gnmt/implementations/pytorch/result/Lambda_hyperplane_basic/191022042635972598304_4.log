Beginning trial 4 of 10
Gathering sys log on lambda-server
:::MLL 1571747682.211 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 178}}
:::MLL 1571747682.212 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 183}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1571747682.213 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 187}}
:::MLL 1571747682.214 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 191}}
:::MLL 1571747682.214 submission_platform: {"value": "1xG481-S80-00", "metadata": {"file": "mlperf_log_utils.py", "lineno": 195}}
:::MLL 1571747682.215 submission_entry: {"value": "{'hardware': 'G481-S80-00', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': ' ', 'os': 'Ubuntu 18.04.3 LTS / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz', 'num_cores': '40', 'num_vcpus': '80', 'accelerator': 'Tesla V100-SXM2-32GB', 'num_accelerators': '8', 'sys_mem_size': '502 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '1x 1.8T', 'cpu_accel_interconnect': 'UPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 199}}
:::MLL 1571747682.216 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 203}}
:::MLL 1571747682.217 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 207}}
Clearing caches
:::MLL 1571747685.026 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node lambda-server
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4565' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191022042635972598304 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191022042635972598304 ./run_and_time.sh
Run vars: id 191022042635972598304 gpus 8 mparams  --master_port=4565
STARTING TIMING RUN AT 2019-10-22 12:34:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --master_port=4565'
running benchmark
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --master_port=4565 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1571747687.605 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571747687.606 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571747687.607 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571747687.612 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571747687.619 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571747687.637 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571747687.642 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571747687.686 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2912393806
0: Worker 0 is using worker seed: 4106998242
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1571747708.040 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1571747710.046 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1571747710.046 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1571747710.046 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1571747710.418 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1571747710.420 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1571747710.420 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1571747710.421 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1571747710.421 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1571747710.421 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1571747710.422 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1571747710.422 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1571747710.441 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571747710.442 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3010504282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 0.531 (0.531)	Data 4.08e-01 (4.08e-01)	Tok/s 19657 (19657)	Loss/tok 10.5823 (10.5823)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.182 (0.228)	Data 1.13e-04 (3.72e-02)	Tok/s 92757 (78549)	Loss/tok 9.6994 (10.1337)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.124 (0.198)	Data 1.46e-04 (1.96e-02)	Tok/s 82890 (83174)	Loss/tok 9.2032 (9.8257)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.125 (0.179)	Data 2.10e-04 (1.33e-02)	Tok/s 82056 (83365)	Loss/tok 8.9338 (9.6373)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.127 (0.173)	Data 2.11e-04 (1.01e-02)	Tok/s 81494 (84224)	Loss/tok 8.6284 (9.4595)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.126 (0.171)	Data 1.54e-04 (8.16e-03)	Tok/s 82719 (84768)	Loss/tok 8.4922 (9.3066)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.126 (0.171)	Data 1.29e-04 (6.85e-03)	Tok/s 81692 (85223)	Loss/tok 8.2301 (9.1644)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.182 (0.166)	Data 1.37e-04 (5.91e-03)	Tok/s 92038 (85048)	Loss/tok 8.2893 (9.0543)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.128 (0.167)	Data 9.58e-05 (5.20e-03)	Tok/s 79128 (85500)	Loss/tok 7.9688 (8.9326)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.127 (0.166)	Data 1.70e-04 (4.65e-03)	Tok/s 82466 (85641)	Loss/tok 7.8362 (8.8318)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.128 (0.167)	Data 1.19e-04 (4.20e-03)	Tok/s 79985 (85817)	Loss/tok 7.7871 (8.7434)	LR 2.000e-04
0: TRAIN [0][110/1938]	Time 0.126 (0.167)	Data 2.56e-04 (3.84e-03)	Tok/s 82281 (85862)	Loss/tok 7.7933 (8.6701)	LR 2.518e-04
0: TRAIN [0][120/1938]	Time 0.126 (0.166)	Data 1.68e-04 (3.53e-03)	Tok/s 79893 (85768)	Loss/tok 7.7752 (8.6112)	LR 3.170e-04
0: TRAIN [0][130/1938]	Time 0.073 (0.165)	Data 2.48e-04 (3.27e-03)	Tok/s 72251 (85739)	Loss/tok 7.0081 (8.5538)	LR 3.991e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][140/1938]	Time 0.127 (0.164)	Data 1.98e-04 (3.05e-03)	Tok/s 81631 (85684)	Loss/tok 7.7080 (8.5092)	LR 4.798e-04
0: TRAIN [0][150/1938]	Time 0.126 (0.163)	Data 2.14e-04 (2.86e-03)	Tok/s 82246 (85579)	Loss/tok 7.5318 (8.4624)	LR 6.040e-04
0: TRAIN [0][160/1938]	Time 0.240 (0.163)	Data 1.05e-04 (2.70e-03)	Tok/s 98321 (85653)	Loss/tok 7.8025 (8.4143)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.071 (0.162)	Data 9.61e-05 (2.55e-03)	Tok/s 75580 (85539)	Loss/tok 6.5751 (8.3679)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.127 (0.160)	Data 9.51e-05 (2.41e-03)	Tok/s 81251 (85349)	Loss/tok 7.2871 (8.3254)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.073 (0.161)	Data 2.51e-04 (2.30e-03)	Tok/s 71166 (85464)	Loss/tok 6.3490 (8.2710)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.240 (0.164)	Data 1.76e-04 (2.19e-03)	Tok/s 96719 (85721)	Loss/tok 7.3034 (8.2135)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.130 (0.165)	Data 1.13e-04 (2.09e-03)	Tok/s 78260 (85865)	Loss/tok 6.6909 (8.1498)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.127 (0.166)	Data 1.76e-04 (2.01e-03)	Tok/s 82093 (85913)	Loss/tok 6.6575 (8.0884)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.307 (0.166)	Data 1.31e-04 (1.93e-03)	Tok/s 97269 (85914)	Loss/tok 6.8424 (8.0293)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.244 (0.166)	Data 1.69e-04 (1.85e-03)	Tok/s 94284 (85996)	Loss/tok 6.7468 (7.9653)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.129 (0.165)	Data 1.26e-04 (1.78e-03)	Tok/s 79671 (85832)	Loss/tok 6.3047 (7.9145)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.245 (0.166)	Data 1.30e-04 (1.72e-03)	Tok/s 95618 (85944)	Loss/tok 6.4969 (7.8457)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.241 (0.166)	Data 1.05e-04 (1.66e-03)	Tok/s 96795 (85875)	Loss/tok 6.5311 (7.7915)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.129 (0.166)	Data 1.54e-04 (1.61e-03)	Tok/s 80988 (85825)	Loss/tok 5.8002 (7.7349)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.186 (0.166)	Data 1.07e-04 (1.56e-03)	Tok/s 89456 (85853)	Loss/tok 5.9849 (7.6749)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.128 (0.165)	Data 1.06e-04 (1.51e-03)	Tok/s 82011 (85855)	Loss/tok 5.5869 (7.6188)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.072 (0.164)	Data 1.07e-04 (1.47e-03)	Tok/s 73964 (85732)	Loss/tok 4.6848 (7.5699)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.311 (0.164)	Data 1.68e-04 (1.43e-03)	Tok/s 94125 (85617)	Loss/tok 6.2778 (7.5198)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.186 (0.165)	Data 1.08e-04 (1.39e-03)	Tok/s 91532 (85712)	Loss/tok 5.6814 (7.4568)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.185 (0.165)	Data 1.08e-04 (1.35e-03)	Tok/s 90842 (85818)	Loss/tok 5.5693 (7.3966)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.309 (0.166)	Data 1.28e-04 (1.32e-03)	Tok/s 95227 (85827)	Loss/tok 6.0074 (7.3425)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.129 (0.165)	Data 1.02e-04 (1.28e-03)	Tok/s 81080 (85790)	Loss/tok 5.1298 (7.2922)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.191 (0.166)	Data 1.70e-04 (1.25e-03)	Tok/s 87775 (85870)	Loss/tok 5.3811 (7.2356)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.129 (0.166)	Data 1.20e-04 (1.22e-03)	Tok/s 82269 (85867)	Loss/tok 4.8110 (7.1815)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.128 (0.166)	Data 1.44e-04 (1.20e-03)	Tok/s 79787 (85872)	Loss/tok 4.8106 (7.1311)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][400/1938]	Time 0.127 (0.165)	Data 2.67e-04 (1.17e-03)	Tok/s 81972 (85772)	Loss/tok 4.7973 (7.0880)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.128 (0.165)	Data 1.36e-04 (1.15e-03)	Tok/s 79660 (85740)	Loss/tok 4.7939 (7.0424)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.071 (0.165)	Data 1.39e-04 (1.12e-03)	Tok/s 73965 (85736)	Loss/tok 3.8340 (6.9944)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.185 (0.165)	Data 1.11e-04 (1.10e-03)	Tok/s 91268 (85684)	Loss/tok 5.0044 (6.9496)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.314 (0.166)	Data 1.68e-04 (1.08e-03)	Tok/s 95177 (85806)	Loss/tok 5.3882 (6.8925)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.129 (0.166)	Data 1.77e-04 (1.06e-03)	Tok/s 80908 (85798)	Loss/tok 4.5045 (6.8434)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.186 (0.166)	Data 1.05e-04 (1.04e-03)	Tok/s 90028 (85849)	Loss/tok 4.7737 (6.7948)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.128 (0.166)	Data 1.50e-04 (1.02e-03)	Tok/s 82415 (85829)	Loss/tok 4.3689 (6.7542)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.127 (0.166)	Data 1.73e-04 (1.00e-03)	Tok/s 78818 (85802)	Loss/tok 4.3377 (6.7131)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.310 (0.166)	Data 1.59e-04 (9.82e-04)	Tok/s 97158 (85784)	Loss/tok 4.9828 (6.6714)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.128 (0.166)	Data 1.42e-04 (9.66e-04)	Tok/s 80202 (85846)	Loss/tok 4.3193 (6.6256)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.185 (0.166)	Data 1.39e-04 (9.49e-04)	Tok/s 89932 (85873)	Loss/tok 4.5090 (6.5824)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.244 (0.166)	Data 1.60e-04 (9.34e-04)	Tok/s 95931 (85858)	Loss/tok 4.6396 (6.5427)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.072 (0.165)	Data 2.59e-04 (9.21e-04)	Tok/s 71585 (85768)	Loss/tok 3.4882 (6.5113)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.311 (0.166)	Data 1.62e-04 (9.07e-04)	Tok/s 94963 (85859)	Loss/tok 4.9345 (6.4655)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.127 (0.166)	Data 1.69e-04 (8.94e-04)	Tok/s 80388 (85824)	Loss/tok 4.1567 (6.4284)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.127 (0.166)	Data 1.68e-04 (8.82e-04)	Tok/s 81442 (85821)	Loss/tok 4.0851 (6.3916)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.183 (0.166)	Data 1.73e-04 (8.71e-04)	Tok/s 91649 (85865)	Loss/tok 4.4159 (6.3531)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.241 (0.166)	Data 2.25e-04 (8.59e-04)	Tok/s 97554 (85846)	Loss/tok 4.7054 (6.3205)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.129 (0.165)	Data 2.18e-04 (8.49e-04)	Tok/s 79993 (85774)	Loss/tok 4.0045 (6.2920)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.071 (0.165)	Data 2.62e-04 (8.38e-04)	Tok/s 74311 (85807)	Loss/tok 3.2409 (6.2573)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.070 (0.165)	Data 2.19e-04 (8.28e-04)	Tok/s 74731 (85774)	Loss/tok 3.3766 (6.2280)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.242 (0.165)	Data 2.04e-04 (8.18e-04)	Tok/s 96817 (85816)	Loss/tok 4.4638 (6.1927)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.186 (0.165)	Data 1.91e-04 (8.08e-04)	Tok/s 90577 (85798)	Loss/tok 4.2650 (6.1628)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.244 (0.165)	Data 1.70e-04 (7.98e-04)	Tok/s 94492 (85792)	Loss/tok 4.4986 (6.1327)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.184 (0.165)	Data 2.59e-04 (7.89e-04)	Tok/s 91636 (85804)	Loss/tok 4.2031 (6.1027)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.073 (0.166)	Data 1.51e-04 (7.80e-04)	Tok/s 71379 (85832)	Loss/tok 3.2476 (6.0714)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][670/1938]	Time 0.187 (0.165)	Data 2.52e-04 (7.71e-04)	Tok/s 90359 (85836)	Loss/tok 4.2169 (6.0430)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.072 (0.165)	Data 2.59e-04 (7.62e-04)	Tok/s 72907 (85812)	Loss/tok 3.1489 (6.0169)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.128 (0.166)	Data 1.39e-04 (7.54e-04)	Tok/s 79194 (85803)	Loss/tok 3.7640 (5.9897)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.125 (0.166)	Data 2.19e-04 (7.46e-04)	Tok/s 83544 (85847)	Loss/tok 3.8144 (5.9612)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.245 (0.166)	Data 2.59e-04 (7.38e-04)	Tok/s 95804 (85892)	Loss/tok 4.3521 (5.9317)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.184 (0.166)	Data 1.81e-04 (7.31e-04)	Tok/s 91147 (85891)	Loss/tok 4.0152 (5.9042)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.129 (0.166)	Data 1.73e-04 (7.24e-04)	Tok/s 79773 (85884)	Loss/tok 3.8058 (5.8803)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.185 (0.166)	Data 1.99e-04 (7.17e-04)	Tok/s 90287 (85885)	Loss/tok 4.2387 (5.8558)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.130 (0.166)	Data 1.70e-04 (7.10e-04)	Tok/s 79508 (85859)	Loss/tok 3.7296 (5.8328)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.074 (0.166)	Data 2.55e-04 (7.03e-04)	Tok/s 70648 (85864)	Loss/tok 3.1565 (5.8087)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.185 (0.166)	Data 1.92e-04 (6.97e-04)	Tok/s 90787 (85854)	Loss/tok 4.0029 (5.7859)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.240 (0.166)	Data 2.18e-04 (6.90e-04)	Tok/s 98240 (85879)	Loss/tok 4.0868 (5.7609)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.128 (0.166)	Data 2.57e-04 (6.84e-04)	Tok/s 80103 (85872)	Loss/tok 3.7209 (5.7396)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.183 (0.166)	Data 2.52e-04 (6.79e-04)	Tok/s 91162 (85873)	Loss/tok 4.0049 (5.7187)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.078 (0.166)	Data 1.67e-04 (6.73e-04)	Tok/s 66246 (85846)	Loss/tok 3.2026 (5.6989)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.129 (0.165)	Data 2.54e-04 (6.67e-04)	Tok/s 79512 (85794)	Loss/tok 3.7308 (5.6809)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.128 (0.165)	Data 2.56e-04 (6.62e-04)	Tok/s 80508 (85772)	Loss/tok 3.5979 (5.6622)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.127 (0.165)	Data 2.05e-04 (6.57e-04)	Tok/s 82583 (85744)	Loss/tok 3.5745 (5.6439)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.075 (0.165)	Data 2.55e-04 (6.52e-04)	Tok/s 70225 (85722)	Loss/tok 3.1209 (5.6249)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.185 (0.165)	Data 1.82e-04 (6.46e-04)	Tok/s 90671 (85731)	Loss/tok 3.9149 (5.6047)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.125 (0.164)	Data 1.71e-04 (6.41e-04)	Tok/s 82323 (85669)	Loss/tok 3.5737 (5.5885)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.183 (0.164)	Data 2.01e-04 (6.36e-04)	Tok/s 91500 (85643)	Loss/tok 3.9930 (5.5714)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.241 (0.164)	Data 2.43e-04 (6.31e-04)	Tok/s 96317 (85651)	Loss/tok 4.0600 (5.5525)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.182 (0.164)	Data 2.55e-04 (6.27e-04)	Tok/s 92556 (85656)	Loss/tok 3.9276 (5.5339)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.182 (0.164)	Data 2.58e-04 (6.23e-04)	Tok/s 91579 (85647)	Loss/tok 3.9096 (5.5163)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.185 (0.164)	Data 1.70e-04 (6.18e-04)	Tok/s 90484 (85689)	Loss/tok 3.8708 (5.4947)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.183 (0.164)	Data 2.54e-04 (6.14e-04)	Tok/s 92964 (85706)	Loss/tok 3.8166 (5.4765)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.132 (0.165)	Data 2.61e-04 (6.10e-04)	Tok/s 77712 (85718)	Loss/tok 3.5919 (5.4582)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.308 (0.165)	Data 2.55e-04 (6.06e-04)	Tok/s 97147 (85730)	Loss/tok 4.3610 (5.4403)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.185 (0.165)	Data 2.62e-04 (6.02e-04)	Tok/s 91365 (85700)	Loss/tok 3.9485 (5.4255)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][970/1938]	Time 0.129 (0.165)	Data 2.62e-04 (5.99e-04)	Tok/s 81430 (85710)	Loss/tok 3.5685 (5.4093)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.184 (0.165)	Data 2.60e-04 (5.95e-04)	Tok/s 91908 (85746)	Loss/tok 3.8872 (5.3920)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.239 (0.165)	Data 2.31e-04 (5.91e-04)	Tok/s 96661 (85728)	Loss/tok 4.1511 (5.3774)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.186 (0.165)	Data 1.65e-04 (5.87e-04)	Tok/s 88709 (85724)	Loss/tok 3.8806 (5.3618)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.129 (0.165)	Data 2.58e-04 (5.84e-04)	Tok/s 79366 (85731)	Loss/tok 3.5118 (5.3465)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.183 (0.165)	Data 2.54e-04 (5.80e-04)	Tok/s 92666 (85758)	Loss/tok 3.7556 (5.3297)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.127 (0.165)	Data 2.21e-04 (5.77e-04)	Tok/s 82603 (85799)	Loss/tok 3.5012 (5.3128)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.186 (0.165)	Data 2.19e-04 (5.73e-04)	Tok/s 89695 (85820)	Loss/tok 3.7941 (5.2970)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.182 (0.165)	Data 2.53e-04 (5.70e-04)	Tok/s 92547 (85838)	Loss/tok 3.8388 (5.2826)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.182 (0.166)	Data 2.11e-04 (5.66e-04)	Tok/s 93143 (85862)	Loss/tok 3.7817 (5.2674)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.184 (0.165)	Data 1.62e-04 (5.63e-04)	Tok/s 90340 (85860)	Loss/tok 3.7932 (5.2538)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.180 (0.166)	Data 1.68e-04 (5.60e-04)	Tok/s 94086 (85891)	Loss/tok 3.6873 (5.2392)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.128 (0.166)	Data 1.93e-04 (5.56e-04)	Tok/s 82174 (85887)	Loss/tok 3.5962 (5.2262)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.182 (0.166)	Data 2.05e-04 (5.53e-04)	Tok/s 92389 (85904)	Loss/tok 3.8188 (5.2131)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1110/1938]	Time 0.311 (0.165)	Data 1.70e-04 (5.50e-04)	Tok/s 95497 (85883)	Loss/tok 4.1324 (5.2010)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.128 (0.166)	Data 1.72e-04 (5.47e-04)	Tok/s 80081 (85891)	Loss/tok 3.6321 (5.1879)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.185 (0.166)	Data 2.26e-04 (5.44e-04)	Tok/s 89108 (85886)	Loss/tok 3.7084 (5.1755)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.186 (0.166)	Data 1.72e-04 (5.41e-04)	Tok/s 91068 (85894)	Loss/tok 3.6744 (5.1627)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.240 (0.166)	Data 1.82e-04 (5.38e-04)	Tok/s 95795 (85903)	Loss/tok 3.9645 (5.1503)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.075 (0.166)	Data 1.26e-04 (5.35e-04)	Tok/s 71806 (85915)	Loss/tok 2.9454 (5.1370)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.129 (0.166)	Data 1.73e-04 (5.32e-04)	Tok/s 78423 (85905)	Loss/tok 3.4726 (5.1257)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.239 (0.166)	Data 1.73e-04 (5.29e-04)	Tok/s 96944 (85895)	Loss/tok 3.9173 (5.1145)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.128 (0.165)	Data 1.77e-04 (5.26e-04)	Tok/s 82875 (85896)	Loss/tok 3.4270 (5.1033)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.184 (0.166)	Data 2.93e-04 (5.23e-04)	Tok/s 92497 (85925)	Loss/tok 3.6924 (5.0901)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.128 (0.166)	Data 1.72e-04 (5.21e-04)	Tok/s 79425 (85920)	Loss/tok 3.5778 (5.0789)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.243 (0.166)	Data 2.61e-04 (5.18e-04)	Tok/s 95761 (85920)	Loss/tok 3.8955 (5.0683)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.185 (0.166)	Data 3.19e-04 (5.15e-04)	Tok/s 90382 (85927)	Loss/tok 3.6238 (5.0568)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.072 (0.166)	Data 1.73e-04 (5.13e-04)	Tok/s 73792 (85931)	Loss/tok 2.9199 (5.0458)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.072 (0.166)	Data 1.90e-04 (5.10e-04)	Tok/s 74262 (85920)	Loss/tok 3.1505 (5.0356)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.127 (0.165)	Data 2.47e-04 (5.08e-04)	Tok/s 82140 (85903)	Loss/tok 3.4449 (5.0261)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.071 (0.165)	Data 1.71e-04 (5.06e-04)	Tok/s 75583 (85872)	Loss/tok 2.8691 (5.0174)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.244 (0.165)	Data 2.58e-04 (5.03e-04)	Tok/s 94578 (85897)	Loss/tok 4.0099 (5.0065)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.244 (0.165)	Data 2.06e-04 (5.01e-04)	Tok/s 94986 (85911)	Loss/tok 3.7674 (4.9954)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.185 (0.165)	Data 1.70e-04 (4.99e-04)	Tok/s 90562 (85893)	Loss/tok 3.7286 (4.9862)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.188 (0.165)	Data 1.78e-04 (4.96e-04)	Tok/s 88563 (85865)	Loss/tok 3.6374 (4.9778)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.127 (0.165)	Data 2.37e-04 (4.94e-04)	Tok/s 81294 (85868)	Loss/tok 3.4117 (4.9678)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.069 (0.165)	Data 1.70e-04 (4.92e-04)	Tok/s 77132 (85850)	Loss/tok 2.8943 (4.9592)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.128 (0.165)	Data 1.55e-04 (4.89e-04)	Tok/s 82317 (85858)	Loss/tok 3.4441 (4.9494)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.130 (0.165)	Data 1.79e-04 (4.87e-04)	Tok/s 80308 (85860)	Loss/tok 3.3281 (4.9398)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1360/1938]	Time 0.129 (0.165)	Data 2.07e-04 (4.85e-04)	Tok/s 81680 (85868)	Loss/tok 3.4203 (4.9301)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.244 (0.165)	Data 1.81e-04 (4.83e-04)	Tok/s 94657 (85872)	Loss/tok 3.8787 (4.9209)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.184 (0.165)	Data 1.70e-04 (4.81e-04)	Tok/s 90627 (85850)	Loss/tok 3.7196 (4.9126)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.187 (0.165)	Data 1.67e-04 (4.78e-04)	Tok/s 90252 (85874)	Loss/tok 3.5857 (4.9026)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.184 (0.165)	Data 1.70e-04 (4.76e-04)	Tok/s 90865 (85865)	Loss/tok 3.5784 (4.8941)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.185 (0.165)	Data 2.04e-04 (4.74e-04)	Tok/s 91443 (85844)	Loss/tok 3.5142 (4.8861)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.187 (0.165)	Data 1.76e-04 (4.72e-04)	Tok/s 90013 (85830)	Loss/tok 3.5638 (4.8778)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.184 (0.165)	Data 1.68e-04 (4.70e-04)	Tok/s 92023 (85864)	Loss/tok 3.6741 (4.8678)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.128 (0.165)	Data 1.91e-04 (4.69e-04)	Tok/s 78793 (85830)	Loss/tok 3.4585 (4.8605)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.184 (0.165)	Data 2.39e-04 (4.67e-04)	Tok/s 92033 (85841)	Loss/tok 3.6542 (4.8521)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.125 (0.165)	Data 2.59e-04 (4.65e-04)	Tok/s 83130 (85836)	Loss/tok 3.3327 (4.8440)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.069 (0.165)	Data 1.72e-04 (4.63e-04)	Tok/s 76037 (85833)	Loss/tok 2.8683 (4.8361)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.127 (0.165)	Data 2.62e-04 (4.61e-04)	Tok/s 81460 (85838)	Loss/tok 3.4321 (4.8279)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.127 (0.165)	Data 2.04e-04 (4.60e-04)	Tok/s 82526 (85819)	Loss/tok 3.3059 (4.8209)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.128 (0.165)	Data 1.48e-04 (4.58e-04)	Tok/s 80891 (85829)	Loss/tok 3.3741 (4.8125)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.127 (0.165)	Data 2.35e-04 (4.56e-04)	Tok/s 83177 (85831)	Loss/tok 3.4238 (4.8049)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.128 (0.165)	Data 1.72e-04 (4.55e-04)	Tok/s 79903 (85811)	Loss/tok 3.2362 (4.7982)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.130 (0.165)	Data 1.67e-04 (4.53e-04)	Tok/s 78102 (85814)	Loss/tok 3.4975 (4.7902)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.312 (0.165)	Data 1.72e-04 (4.51e-04)	Tok/s 95938 (85809)	Loss/tok 3.8899 (4.7823)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.125 (0.165)	Data 1.55e-04 (4.49e-04)	Tok/s 85377 (85801)	Loss/tok 3.4965 (4.7750)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.133 (0.165)	Data 1.70e-04 (4.48e-04)	Tok/s 77921 (85812)	Loss/tok 3.3462 (4.7664)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.069 (0.165)	Data 1.71e-04 (4.46e-04)	Tok/s 76318 (85810)	Loss/tok 2.9289 (4.7587)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.184 (0.165)	Data 1.72e-04 (4.44e-04)	Tok/s 92117 (85802)	Loss/tok 3.5874 (4.7517)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.139 (0.165)	Data 1.71e-04 (4.43e-04)	Tok/s 73665 (85781)	Loss/tok 3.4358 (4.7448)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.134 (0.165)	Data 2.60e-04 (4.41e-04)	Tok/s 77516 (85805)	Loss/tok 3.3447 (4.7363)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.130 (0.165)	Data 1.75e-04 (4.39e-04)	Tok/s 77362 (85802)	Loss/tok 3.3275 (4.7292)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1620/1938]	Time 0.188 (0.165)	Data 1.70e-04 (4.38e-04)	Tok/s 89510 (85810)	Loss/tok 3.6553 (4.7218)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.133 (0.165)	Data 1.22e-04 (4.36e-04)	Tok/s 76072 (85807)	Loss/tok 3.3450 (4.7147)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.189 (0.165)	Data 1.75e-04 (4.35e-04)	Tok/s 90094 (85803)	Loss/tok 3.4976 (4.7073)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.075 (0.165)	Data 2.10e-04 (4.33e-04)	Tok/s 70395 (85766)	Loss/tok 2.7388 (4.7011)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.252 (0.165)	Data 1.69e-04 (4.31e-04)	Tok/s 92616 (85767)	Loss/tok 3.8504 (4.6936)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.133 (0.165)	Data 1.70e-04 (4.30e-04)	Tok/s 76063 (85731)	Loss/tok 3.4610 (4.6879)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.134 (0.165)	Data 1.70e-04 (4.28e-04)	Tok/s 79476 (85708)	Loss/tok 3.2639 (4.6817)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.187 (0.165)	Data 1.82e-04 (4.27e-04)	Tok/s 88591 (85712)	Loss/tok 3.6730 (4.6749)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.131 (0.165)	Data 1.72e-04 (4.25e-04)	Tok/s 79529 (85691)	Loss/tok 3.3454 (4.6688)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.135 (0.165)	Data 1.25e-04 (4.24e-04)	Tok/s 78851 (85680)	Loss/tok 3.3546 (4.6624)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.191 (0.165)	Data 1.74e-04 (4.22e-04)	Tok/s 86822 (85681)	Loss/tok 3.6185 (4.6558)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.246 (0.165)	Data 1.72e-04 (4.21e-04)	Tok/s 94418 (85686)	Loss/tok 3.8780 (4.6489)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.246 (0.166)	Data 2.23e-04 (4.20e-04)	Tok/s 93551 (85694)	Loss/tok 3.8056 (4.6422)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.188 (0.166)	Data 2.15e-04 (4.18e-04)	Tok/s 88912 (85686)	Loss/tok 3.6003 (4.6362)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.130 (0.166)	Data 2.61e-04 (4.17e-04)	Tok/s 79524 (85681)	Loss/tok 3.3683 (4.6304)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.133 (0.166)	Data 1.56e-04 (4.16e-04)	Tok/s 77608 (85668)	Loss/tok 3.2605 (4.6243)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.136 (0.166)	Data 1.25e-04 (4.14e-04)	Tok/s 75898 (85644)	Loss/tok 3.2716 (4.6187)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.192 (0.166)	Data 1.30e-04 (4.13e-04)	Tok/s 87499 (85651)	Loss/tok 3.5843 (4.6122)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1800/1938]	Time 0.072 (0.166)	Data 1.77e-04 (4.11e-04)	Tok/s 73628 (85655)	Loss/tok 2.8678 (4.6058)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.134 (0.166)	Data 1.23e-04 (4.10e-04)	Tok/s 77960 (85663)	Loss/tok 3.3768 (4.5995)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.135 (0.166)	Data 1.68e-04 (4.09e-04)	Tok/s 76720 (85668)	Loss/tok 3.3811 (4.5934)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.132 (0.166)	Data 1.73e-04 (4.07e-04)	Tok/s 76971 (85658)	Loss/tok 3.4168 (4.5879)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.072 (0.166)	Data 1.70e-04 (4.06e-04)	Tok/s 72048 (85649)	Loss/tok 2.8053 (4.5821)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.130 (0.166)	Data 1.74e-04 (4.05e-04)	Tok/s 78847 (85652)	Loss/tok 3.3961 (4.5762)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.187 (0.166)	Data 2.22e-04 (4.04e-04)	Tok/s 89967 (85664)	Loss/tok 3.5615 (4.5704)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.130 (0.166)	Data 1.75e-04 (4.03e-04)	Tok/s 80632 (85670)	Loss/tok 3.3287 (4.5649)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.186 (0.166)	Data 1.80e-04 (4.01e-04)	Tok/s 90302 (85679)	Loss/tok 3.6020 (4.5590)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.184 (0.166)	Data 2.45e-04 (4.01e-04)	Tok/s 89855 (85660)	Loss/tok 3.5486 (4.5541)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.243 (0.166)	Data 2.21e-04 (4.00e-04)	Tok/s 96050 (85645)	Loss/tok 3.7325 (4.5492)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.244 (0.166)	Data 2.63e-04 (3.99e-04)	Tok/s 95569 (85657)	Loss/tok 3.7953 (4.5439)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.183 (0.166)	Data 1.73e-04 (3.98e-04)	Tok/s 91386 (85660)	Loss/tok 3.5626 (4.5387)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.130 (0.166)	Data 1.72e-04 (3.97e-04)	Tok/s 79399 (85651)	Loss/tok 3.2115 (4.5335)	LR 2.000e-03
:::MLL 1571748033.159 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1571748033.160 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.673 (0.673)	Decoder iters 124.0 (124.0)	Tok/s 24034 (24034)
0: Running moses detokenizer
0: BLEU(score=20.249256153916107, counts=[34530, 15924, 8495, 4708], totals=[64736, 61733, 58730, 55730], precisions=[53.339718240237275, 25.794955696305056, 14.46449855269879, 8.447873676655302], bp=1.0, sys_len=64736, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571748035.119 eval_accuracy: {"value": 20.25, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1571748035.119 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.5264	Test BLEU: 20.25
0: Performance: Epoch: 0	Training: 685201 Tok/s
0: Finished epoch 0
:::MLL 1571748035.120 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1571748035.120 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571748035.120 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 71275948
0: TRAIN [1][0/1938]	Time 0.559 (0.559)	Data 3.40e-01 (3.40e-01)	Tok/s 29997 (29997)	Loss/tok 3.4934 (3.4934)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.134 (0.191)	Data 1.21e-04 (3.11e-02)	Tok/s 79071 (77836)	Loss/tok 3.2409 (3.4393)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.187 (0.173)	Data 1.76e-04 (1.64e-02)	Tok/s 89770 (80854)	Loss/tok 3.4708 (3.4222)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.129 (0.166)	Data 2.69e-04 (1.12e-02)	Tok/s 81245 (81616)	Loss/tok 3.2544 (3.4104)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.316 (0.165)	Data 1.73e-04 (8.47e-03)	Tok/s 94704 (82173)	Loss/tok 3.7914 (3.4149)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.184 (0.167)	Data 1.52e-04 (6.84e-03)	Tok/s 90604 (83081)	Loss/tok 3.5440 (3.4329)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.131 (0.168)	Data 1.39e-04 (5.75e-03)	Tok/s 79479 (83336)	Loss/tok 3.1970 (3.4386)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.183 (0.166)	Data 1.07e-04 (4.96e-03)	Tok/s 92163 (83576)	Loss/tok 3.5232 (3.4357)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.130 (0.167)	Data 1.67e-04 (4.36e-03)	Tok/s 79542 (83854)	Loss/tok 3.2936 (3.4443)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.185 (0.168)	Data 1.05e-04 (3.90e-03)	Tok/s 89543 (84356)	Loss/tok 3.4404 (3.4462)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.128 (0.164)	Data 1.12e-04 (3.52e-03)	Tok/s 79596 (83997)	Loss/tok 3.2286 (3.4314)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.246 (0.168)	Data 1.44e-04 (3.22e-03)	Tok/s 95113 (84496)	Loss/tok 3.6551 (3.4530)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.241 (0.166)	Data 2.01e-04 (2.97e-03)	Tok/s 98263 (84436)	Loss/tok 3.5510 (3.4483)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][130/1938]	Time 0.241 (0.165)	Data 1.04e-04 (2.75e-03)	Tok/s 98058 (84381)	Loss/tok 3.6074 (3.4457)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.130 (0.167)	Data 1.09e-04 (2.56e-03)	Tok/s 80755 (84678)	Loss/tok 3.2288 (3.4525)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.128 (0.166)	Data 1.54e-04 (2.40e-03)	Tok/s 79731 (84613)	Loss/tok 3.3490 (3.4467)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.183 (0.165)	Data 1.39e-04 (2.26e-03)	Tok/s 90917 (84601)	Loss/tok 3.3864 (3.4429)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.130 (0.164)	Data 1.75e-04 (2.14e-03)	Tok/s 79402 (84620)	Loss/tok 3.3122 (3.4395)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.182 (0.165)	Data 1.51e-04 (2.03e-03)	Tok/s 92233 (84776)	Loss/tok 3.5111 (3.4401)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.075 (0.165)	Data 1.86e-04 (1.93e-03)	Tok/s 70386 (84783)	Loss/tok 2.7527 (3.4383)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.312 (0.165)	Data 1.24e-04 (1.85e-03)	Tok/s 96487 (84913)	Loss/tok 3.7664 (3.4439)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.180 (0.166)	Data 1.05e-04 (1.77e-03)	Tok/s 92541 (85047)	Loss/tok 3.5299 (3.4488)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.075 (0.165)	Data 1.51e-04 (1.69e-03)	Tok/s 70860 (84904)	Loss/tok 2.7616 (3.4450)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.129 (0.164)	Data 1.08e-04 (1.63e-03)	Tok/s 80851 (84835)	Loss/tok 3.1745 (3.4428)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.248 (0.165)	Data 9.54e-05 (1.56e-03)	Tok/s 93836 (84922)	Loss/tok 3.6669 (3.4446)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.129 (0.164)	Data 1.55e-04 (1.51e-03)	Tok/s 82261 (84828)	Loss/tok 3.1270 (3.4407)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.128 (0.163)	Data 1.08e-04 (1.46e-03)	Tok/s 80747 (84802)	Loss/tok 3.1072 (3.4373)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][270/1938]	Time 0.130 (0.164)	Data 1.10e-04 (1.41e-03)	Tok/s 80482 (84929)	Loss/tok 3.1838 (3.4441)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.130 (0.165)	Data 1.15e-04 (1.36e-03)	Tok/s 80703 (84972)	Loss/tok 3.2144 (3.4487)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.313 (0.166)	Data 1.79e-04 (1.32e-03)	Tok/s 93389 (85154)	Loss/tok 4.0723 (3.4550)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.313 (0.167)	Data 2.56e-04 (1.28e-03)	Tok/s 95970 (85316)	Loss/tok 3.7690 (3.4578)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.127 (0.167)	Data 1.14e-04 (1.25e-03)	Tok/s 81547 (85406)	Loss/tok 3.2349 (3.4562)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.127 (0.167)	Data 2.02e-04 (1.21e-03)	Tok/s 81459 (85335)	Loss/tok 3.2275 (3.4562)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][330/1938]	Time 0.126 (0.166)	Data 2.03e-04 (1.18e-03)	Tok/s 81464 (85249)	Loss/tok 3.2225 (3.4562)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.074 (0.166)	Data 1.68e-04 (1.15e-03)	Tok/s 71354 (85305)	Loss/tok 2.8284 (3.4561)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.127 (0.167)	Data 1.65e-04 (1.12e-03)	Tok/s 83354 (85352)	Loss/tok 3.1994 (3.4581)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.131 (0.166)	Data 1.52e-04 (1.09e-03)	Tok/s 81332 (85310)	Loss/tok 3.2536 (3.4568)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.183 (0.166)	Data 1.24e-04 (1.07e-03)	Tok/s 91294 (85272)	Loss/tok 3.3800 (3.4542)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.186 (0.166)	Data 1.15e-04 (1.04e-03)	Tok/s 90267 (85340)	Loss/tok 3.4403 (3.4559)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.183 (0.166)	Data 1.53e-04 (1.02e-03)	Tok/s 92929 (85378)	Loss/tok 3.4942 (3.4550)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.129 (0.166)	Data 1.05e-04 (9.97e-04)	Tok/s 79689 (85330)	Loss/tok 3.1739 (3.4561)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.247 (0.167)	Data 9.42e-05 (9.76e-04)	Tok/s 94192 (85393)	Loss/tok 3.5788 (3.4558)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.246 (0.166)	Data 1.68e-04 (9.58e-04)	Tok/s 93366 (85363)	Loss/tok 3.7134 (3.4558)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.183 (0.166)	Data 1.13e-04 (9.40e-04)	Tok/s 93816 (85357)	Loss/tok 3.3673 (3.4541)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.127 (0.166)	Data 1.42e-04 (9.22e-04)	Tok/s 79220 (85317)	Loss/tok 3.2440 (3.4520)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.128 (0.166)	Data 1.02e-04 (9.04e-04)	Tok/s 79512 (85385)	Loss/tok 3.2505 (3.4535)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.126 (0.166)	Data 1.22e-04 (8.88e-04)	Tok/s 83790 (85397)	Loss/tok 3.2361 (3.4525)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.315 (0.167)	Data 1.53e-04 (8.72e-04)	Tok/s 94848 (85437)	Loss/tok 3.8028 (3.4546)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.131 (0.167)	Data 1.73e-04 (8.58e-04)	Tok/s 79947 (85463)	Loss/tok 3.3750 (3.4555)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.125 (0.166)	Data 2.59e-04 (8.44e-04)	Tok/s 82032 (85432)	Loss/tok 3.0745 (3.4520)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.128 (0.166)	Data 1.10e-04 (8.30e-04)	Tok/s 79615 (85441)	Loss/tok 3.3307 (3.4519)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.124 (0.166)	Data 1.80e-04 (8.17e-04)	Tok/s 82085 (85476)	Loss/tok 3.2329 (3.4525)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][520/1938]	Time 0.191 (0.167)	Data 1.40e-04 (8.05e-04)	Tok/s 89037 (85491)	Loss/tok 3.3732 (3.4525)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.190 (0.167)	Data 1.67e-04 (7.92e-04)	Tok/s 89276 (85561)	Loss/tok 3.4764 (3.4525)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.315 (0.167)	Data 1.42e-04 (7.81e-04)	Tok/s 95874 (85598)	Loss/tok 3.6120 (3.4537)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.072 (0.168)	Data 1.41e-04 (7.69e-04)	Tok/s 74428 (85597)	Loss/tok 2.7954 (3.4549)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.241 (0.168)	Data 1.68e-04 (7.58e-04)	Tok/s 96376 (85656)	Loss/tok 3.6642 (3.4557)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.072 (0.168)	Data 9.63e-05 (7.47e-04)	Tok/s 73628 (85641)	Loss/tok 2.7063 (3.4560)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.125 (0.167)	Data 1.09e-04 (7.37e-04)	Tok/s 83684 (85647)	Loss/tok 3.1654 (3.4540)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.187 (0.167)	Data 1.77e-04 (7.27e-04)	Tok/s 90041 (85683)	Loss/tok 3.3558 (3.4527)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.187 (0.168)	Data 1.49e-04 (7.17e-04)	Tok/s 89419 (85695)	Loss/tok 3.4479 (3.4524)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.074 (0.168)	Data 1.67e-04 (7.08e-04)	Tok/s 72408 (85682)	Loss/tok 2.7283 (3.4508)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.188 (0.168)	Data 1.44e-04 (6.99e-04)	Tok/s 87893 (85675)	Loss/tok 3.4596 (3.4498)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.128 (0.167)	Data 1.80e-04 (6.91e-04)	Tok/s 80647 (85638)	Loss/tok 3.1575 (3.4484)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.124 (0.168)	Data 2.66e-04 (6.83e-04)	Tok/s 83772 (85683)	Loss/tok 3.2649 (3.4499)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.184 (0.168)	Data 1.52e-04 (6.75e-04)	Tok/s 90546 (85702)	Loss/tok 3.3731 (3.4502)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.128 (0.168)	Data 1.69e-04 (6.67e-04)	Tok/s 81048 (85759)	Loss/tok 3.1403 (3.4512)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.240 (0.168)	Data 1.67e-04 (6.60e-04)	Tok/s 96559 (85780)	Loss/tok 3.5192 (3.4511)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.128 (0.168)	Data 1.65e-04 (6.52e-04)	Tok/s 80427 (85785)	Loss/tok 3.0984 (3.4500)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.128 (0.168)	Data 1.49e-04 (6.45e-04)	Tok/s 79959 (85789)	Loss/tok 3.1605 (3.4484)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.126 (0.168)	Data 1.08e-04 (6.38e-04)	Tok/s 82096 (85800)	Loss/tok 3.1359 (3.4481)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.125 (0.168)	Data 2.36e-04 (6.32e-04)	Tok/s 81140 (85797)	Loss/tok 3.2709 (3.4465)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.238 (0.168)	Data 2.59e-04 (6.25e-04)	Tok/s 99414 (85766)	Loss/tok 3.5956 (3.4470)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.071 (0.168)	Data 1.44e-04 (6.19e-04)	Tok/s 74990 (85737)	Loss/tok 2.8532 (3.4455)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.128 (0.168)	Data 1.69e-04 (6.13e-04)	Tok/s 80161 (85774)	Loss/tok 3.1875 (3.4457)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.188 (0.168)	Data 1.79e-04 (6.08e-04)	Tok/s 90662 (85794)	Loss/tok 3.4430 (3.4454)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.183 (0.168)	Data 1.68e-04 (6.02e-04)	Tok/s 91682 (85811)	Loss/tok 3.3696 (3.4444)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.243 (0.168)	Data 1.42e-04 (5.96e-04)	Tok/s 96704 (85824)	Loss/tok 3.6497 (3.4453)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.125 (0.168)	Data 1.68e-04 (5.90e-04)	Tok/s 81713 (85828)	Loss/tok 3.0995 (3.4452)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][790/1938]	Time 0.130 (0.168)	Data 1.10e-04 (5.85e-04)	Tok/s 79475 (85816)	Loss/tok 3.3174 (3.4444)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.127 (0.168)	Data 1.70e-04 (5.80e-04)	Tok/s 80218 (85774)	Loss/tok 3.1178 (3.4426)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.130 (0.168)	Data 1.38e-04 (5.74e-04)	Tok/s 78577 (85773)	Loss/tok 3.1700 (3.4419)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.191 (0.168)	Data 1.70e-04 (5.69e-04)	Tok/s 87690 (85785)	Loss/tok 3.3551 (3.4414)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.188 (0.168)	Data 1.71e-04 (5.64e-04)	Tok/s 89571 (85785)	Loss/tok 3.3193 (3.4402)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.130 (0.168)	Data 1.39e-04 (5.60e-04)	Tok/s 77416 (85784)	Loss/tok 3.1672 (3.4402)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.187 (0.167)	Data 1.34e-04 (5.55e-04)	Tok/s 89734 (85759)	Loss/tok 3.3641 (3.4390)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.072 (0.167)	Data 1.10e-04 (5.50e-04)	Tok/s 74147 (85752)	Loss/tok 2.7201 (3.4380)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.129 (0.168)	Data 2.37e-04 (5.45e-04)	Tok/s 77292 (85761)	Loss/tok 3.2354 (3.4382)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.241 (0.168)	Data 2.57e-04 (5.41e-04)	Tok/s 96156 (85806)	Loss/tok 3.6937 (3.4383)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.184 (0.168)	Data 1.41e-04 (5.37e-04)	Tok/s 91964 (85773)	Loss/tok 3.3429 (3.4367)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.072 (0.168)	Data 1.15e-04 (5.33e-04)	Tok/s 76223 (85773)	Loss/tok 2.6590 (3.4359)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.130 (0.168)	Data 1.20e-04 (5.28e-04)	Tok/s 79864 (85801)	Loss/tok 3.2341 (3.4358)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][920/1938]	Time 0.073 (0.168)	Data 1.33e-04 (5.24e-04)	Tok/s 69615 (85789)	Loss/tok 2.6899 (3.4354)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.185 (0.168)	Data 1.09e-04 (5.20e-04)	Tok/s 90549 (85801)	Loss/tok 3.4978 (3.4353)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.128 (0.168)	Data 2.56e-04 (5.16e-04)	Tok/s 79394 (85786)	Loss/tok 3.1578 (3.4342)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.182 (0.168)	Data 1.13e-04 (5.13e-04)	Tok/s 92334 (85833)	Loss/tok 3.4838 (3.4344)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.126 (0.168)	Data 2.55e-04 (5.09e-04)	Tok/s 81749 (85794)	Loss/tok 3.2025 (3.4334)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.129 (0.168)	Data 1.70e-04 (5.06e-04)	Tok/s 80055 (85799)	Loss/tok 3.1832 (3.4325)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.248 (0.167)	Data 1.43e-04 (5.02e-04)	Tok/s 93900 (85801)	Loss/tok 3.5996 (3.4318)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.127 (0.168)	Data 2.55e-04 (4.99e-04)	Tok/s 81199 (85789)	Loss/tok 3.2132 (3.4328)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1000/1938]	Time 0.186 (0.168)	Data 1.69e-04 (4.95e-04)	Tok/s 89320 (85820)	Loss/tok 3.4618 (3.4330)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.239 (0.167)	Data 2.01e-04 (4.92e-04)	Tok/s 97351 (85792)	Loss/tok 3.6893 (3.4322)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.127 (0.167)	Data 1.09e-04 (4.89e-04)	Tok/s 82049 (85791)	Loss/tok 3.1446 (3.4315)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.242 (0.168)	Data 1.73e-04 (4.85e-04)	Tok/s 96498 (85840)	Loss/tok 3.5320 (3.4317)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.129 (0.168)	Data 9.25e-05 (4.82e-04)	Tok/s 79369 (85839)	Loss/tok 3.2154 (3.4316)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.128 (0.168)	Data 2.02e-04 (4.79e-04)	Tok/s 81550 (85853)	Loss/tok 3.0706 (3.4309)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.129 (0.167)	Data 1.46e-04 (4.76e-04)	Tok/s 79991 (85840)	Loss/tok 3.1571 (3.4300)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.185 (0.167)	Data 1.60e-04 (4.73e-04)	Tok/s 90366 (85814)	Loss/tok 3.4636 (3.4283)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.128 (0.167)	Data 1.62e-04 (4.70e-04)	Tok/s 81488 (85808)	Loss/tok 3.1710 (3.4275)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.183 (0.167)	Data 1.72e-04 (4.67e-04)	Tok/s 92212 (85809)	Loss/tok 3.3906 (3.4265)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.183 (0.167)	Data 2.02e-04 (4.64e-04)	Tok/s 90609 (85821)	Loss/tok 3.4095 (3.4262)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.240 (0.167)	Data 2.20e-04 (4.61e-04)	Tok/s 96627 (85838)	Loss/tok 3.5869 (3.4265)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.127 (0.167)	Data 1.09e-04 (4.58e-04)	Tok/s 82406 (85853)	Loss/tok 3.2295 (3.4264)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.128 (0.167)	Data 1.07e-04 (4.56e-04)	Tok/s 81207 (85853)	Loss/tok 3.1320 (3.4260)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.242 (0.167)	Data 9.75e-05 (4.53e-04)	Tok/s 97209 (85851)	Loss/tok 3.4253 (3.4254)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.181 (0.167)	Data 1.70e-04 (4.50e-04)	Tok/s 93466 (85844)	Loss/tok 3.3348 (3.4248)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.128 (0.167)	Data 1.06e-04 (4.47e-04)	Tok/s 80492 (85848)	Loss/tok 3.1824 (3.4248)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.184 (0.167)	Data 2.58e-04 (4.45e-04)	Tok/s 90909 (85868)	Loss/tok 3.4552 (3.4241)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.128 (0.167)	Data 1.03e-04 (4.42e-04)	Tok/s 81810 (85877)	Loss/tok 3.1625 (3.4240)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.127 (0.167)	Data 2.57e-04 (4.40e-04)	Tok/s 80263 (85877)	Loss/tok 3.1466 (3.4236)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.181 (0.167)	Data 1.54e-04 (4.38e-04)	Tok/s 92031 (85878)	Loss/tok 3.3662 (3.4230)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.072 (0.167)	Data 1.47e-04 (4.35e-04)	Tok/s 72145 (85886)	Loss/tok 2.7909 (3.4229)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.180 (0.167)	Data 1.34e-04 (4.33e-04)	Tok/s 92010 (85875)	Loss/tok 3.3844 (3.4228)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.127 (0.167)	Data 1.09e-04 (4.31e-04)	Tok/s 80483 (85900)	Loss/tok 3.0654 (3.4225)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.182 (0.167)	Data 2.03e-04 (4.28e-04)	Tok/s 92375 (85919)	Loss/tok 3.3823 (3.4221)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.129 (0.167)	Data 2.41e-04 (4.26e-04)	Tok/s 79196 (85946)	Loss/tok 3.1731 (3.4224)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.186 (0.167)	Data 1.01e-04 (4.24e-04)	Tok/s 92194 (85946)	Loss/tok 3.3290 (3.4219)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.071 (0.167)	Data 2.20e-04 (4.22e-04)	Tok/s 74859 (85959)	Loss/tok 2.7301 (3.4212)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.129 (0.167)	Data 1.10e-04 (4.20e-04)	Tok/s 79843 (85976)	Loss/tok 3.2686 (3.4213)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.126 (0.167)	Data 2.38e-04 (4.18e-04)	Tok/s 81664 (85965)	Loss/tok 3.1684 (3.4205)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.130 (0.167)	Data 1.67e-04 (4.16e-04)	Tok/s 79168 (85947)	Loss/tok 3.1236 (3.4198)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.317 (0.167)	Data 1.40e-04 (4.14e-04)	Tok/s 93857 (85952)	Loss/tok 3.7578 (3.4202)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1320/1938]	Time 0.313 (0.167)	Data 1.09e-04 (4.12e-04)	Tok/s 95929 (85985)	Loss/tok 3.7262 (3.4213)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.127 (0.167)	Data 2.53e-04 (4.10e-04)	Tok/s 81631 (85978)	Loss/tok 3.1291 (3.4210)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1340/1938]	Time 0.241 (0.167)	Data 1.20e-04 (4.08e-04)	Tok/s 96650 (85997)	Loss/tok 3.5420 (3.4217)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.127 (0.167)	Data 1.76e-04 (4.06e-04)	Tok/s 82632 (86000)	Loss/tok 3.1048 (3.4218)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.184 (0.167)	Data 2.56e-04 (4.04e-04)	Tok/s 92346 (86006)	Loss/tok 3.2924 (3.4214)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.124 (0.167)	Data 2.82e-04 (4.03e-04)	Tok/s 82363 (86007)	Loss/tok 3.1250 (3.4208)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.243 (0.167)	Data 1.05e-04 (4.01e-04)	Tok/s 96169 (86019)	Loss/tok 3.4970 (3.4202)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.185 (0.167)	Data 1.08e-04 (3.99e-04)	Tok/s 90730 (86032)	Loss/tok 3.4193 (3.4196)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.183 (0.167)	Data 2.05e-04 (3.97e-04)	Tok/s 91774 (86044)	Loss/tok 3.3628 (3.4195)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.242 (0.167)	Data 1.39e-04 (3.95e-04)	Tok/s 94964 (86029)	Loss/tok 3.6055 (3.4188)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.127 (0.167)	Data 1.15e-04 (3.94e-04)	Tok/s 82293 (86018)	Loss/tok 3.1206 (3.4184)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.125 (0.167)	Data 1.73e-04 (3.92e-04)	Tok/s 83494 (86024)	Loss/tok 3.1424 (3.4178)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.189 (0.167)	Data 1.71e-04 (3.90e-04)	Tok/s 89131 (86033)	Loss/tok 3.3166 (3.4176)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.240 (0.167)	Data 1.68e-04 (3.89e-04)	Tok/s 98279 (86040)	Loss/tok 3.5042 (3.4170)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.068 (0.167)	Data 1.66e-04 (3.87e-04)	Tok/s 75652 (86040)	Loss/tok 2.6159 (3.4166)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.183 (0.167)	Data 2.03e-04 (3.86e-04)	Tok/s 91838 (86052)	Loss/tok 3.2793 (3.4163)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.127 (0.167)	Data 2.32e-04 (3.84e-04)	Tok/s 84139 (86057)	Loss/tok 3.1088 (3.4158)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.128 (0.167)	Data 1.67e-04 (3.82e-04)	Tok/s 82391 (86049)	Loss/tok 3.2149 (3.4147)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.125 (0.167)	Data 1.16e-04 (3.81e-04)	Tok/s 81380 (86059)	Loss/tok 3.1253 (3.4140)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.126 (0.167)	Data 1.71e-04 (3.80e-04)	Tok/s 80938 (86081)	Loss/tok 3.1341 (3.4144)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.315 (0.167)	Data 1.45e-04 (3.78e-04)	Tok/s 93894 (86069)	Loss/tok 3.6771 (3.4138)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.126 (0.167)	Data 1.20e-04 (3.77e-04)	Tok/s 82552 (86078)	Loss/tok 3.1890 (3.4133)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.242 (0.167)	Data 1.88e-04 (3.75e-04)	Tok/s 95570 (86066)	Loss/tok 3.4925 (3.4126)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.184 (0.167)	Data 1.73e-04 (3.74e-04)	Tok/s 91902 (86097)	Loss/tok 3.4027 (3.4126)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.127 (0.167)	Data 1.47e-04 (3.72e-04)	Tok/s 83221 (86111)	Loss/tok 3.1423 (3.4121)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.244 (0.167)	Data 1.84e-04 (3.71e-04)	Tok/s 94929 (86090)	Loss/tok 3.5703 (3.4113)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.187 (0.167)	Data 1.63e-04 (3.70e-04)	Tok/s 89309 (86106)	Loss/tok 3.3085 (3.4112)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.184 (0.167)	Data 1.72e-04 (3.69e-04)	Tok/s 90998 (86118)	Loss/tok 3.4222 (3.4111)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.242 (0.167)	Data 1.67e-04 (3.68e-04)	Tok/s 96669 (86125)	Loss/tok 3.5494 (3.4107)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.129 (0.167)	Data 9.49e-05 (3.66e-04)	Tok/s 80006 (86138)	Loss/tok 3.1082 (3.4105)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.132 (0.167)	Data 1.40e-04 (3.65e-04)	Tok/s 78625 (86134)	Loss/tok 3.0714 (3.4106)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.184 (0.167)	Data 1.20e-04 (3.64e-04)	Tok/s 91180 (86132)	Loss/tok 3.3511 (3.4098)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.183 (0.167)	Data 1.38e-04 (3.62e-04)	Tok/s 91303 (86121)	Loss/tok 3.2987 (3.4091)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.127 (0.167)	Data 1.24e-04 (3.61e-04)	Tok/s 80917 (86134)	Loss/tok 3.1593 (3.4096)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.183 (0.167)	Data 1.68e-04 (3.60e-04)	Tok/s 91657 (86148)	Loss/tok 3.3069 (3.4094)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.184 (0.167)	Data 1.08e-04 (3.59e-04)	Tok/s 90403 (86152)	Loss/tok 3.3912 (3.4090)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.073 (0.167)	Data 1.60e-04 (3.58e-04)	Tok/s 72222 (86128)	Loss/tok 2.7268 (3.4080)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.072 (0.167)	Data 2.34e-04 (3.57e-04)	Tok/s 72487 (86112)	Loss/tok 2.6943 (3.4073)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.126 (0.167)	Data 2.41e-04 (3.56e-04)	Tok/s 83641 (86110)	Loss/tok 3.1373 (3.4070)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.076 (0.166)	Data 1.52e-04 (3.55e-04)	Tok/s 69260 (86083)	Loss/tok 2.5765 (3.4060)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1720/1938]	Time 0.182 (0.166)	Data 1.10e-04 (3.53e-04)	Tok/s 91807 (86072)	Loss/tok 3.3271 (3.4058)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.132 (0.166)	Data 1.08e-04 (3.52e-04)	Tok/s 77931 (86046)	Loss/tok 3.1831 (3.4048)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.185 (0.166)	Data 9.44e-05 (3.51e-04)	Tok/s 90899 (86045)	Loss/tok 3.4040 (3.4043)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.126 (0.166)	Data 1.59e-04 (3.50e-04)	Tok/s 82940 (86047)	Loss/tok 3.2537 (3.4038)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.185 (0.166)	Data 1.69e-04 (3.49e-04)	Tok/s 90978 (86064)	Loss/tok 3.2726 (3.4038)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.184 (0.166)	Data 1.43e-04 (3.48e-04)	Tok/s 92335 (86058)	Loss/tok 3.4245 (3.4033)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.186 (0.166)	Data 1.72e-04 (3.47e-04)	Tok/s 90344 (86064)	Loss/tok 3.2816 (3.4026)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.239 (0.166)	Data 1.27e-04 (3.46e-04)	Tok/s 96950 (86060)	Loss/tok 3.4959 (3.4018)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.240 (0.166)	Data 1.40e-04 (3.45e-04)	Tok/s 96359 (86048)	Loss/tok 3.6149 (3.4018)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.183 (0.166)	Data 1.28e-04 (3.44e-04)	Tok/s 92782 (86048)	Loss/tok 3.3059 (3.4010)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.128 (0.166)	Data 1.80e-04 (3.43e-04)	Tok/s 82754 (86036)	Loss/tok 3.1906 (3.4003)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.245 (0.166)	Data 1.58e-04 (3.42e-04)	Tok/s 96051 (86054)	Loss/tok 3.4507 (3.4001)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.185 (0.166)	Data 1.73e-04 (3.41e-04)	Tok/s 90246 (86069)	Loss/tok 3.3713 (3.3999)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1850/1938]	Time 0.129 (0.166)	Data 1.92e-04 (3.40e-04)	Tok/s 79592 (86072)	Loss/tok 3.0623 (3.4001)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.129 (0.166)	Data 2.37e-04 (3.39e-04)	Tok/s 80453 (86063)	Loss/tok 3.1191 (3.3992)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.189 (0.166)	Data 1.73e-04 (3.38e-04)	Tok/s 88113 (86072)	Loss/tok 3.3564 (3.3989)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.185 (0.166)	Data 2.07e-04 (3.37e-04)	Tok/s 91008 (86074)	Loss/tok 3.3204 (3.3983)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.131 (0.166)	Data 1.71e-04 (3.37e-04)	Tok/s 78665 (86062)	Loss/tok 3.0635 (3.3976)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.130 (0.166)	Data 1.72e-04 (3.36e-04)	Tok/s 81553 (86040)	Loss/tok 3.0213 (3.3975)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.071 (0.166)	Data 2.08e-04 (3.35e-04)	Tok/s 75259 (86009)	Loss/tok 2.6621 (3.3970)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.128 (0.166)	Data 1.69e-04 (3.34e-04)	Tok/s 80537 (85992)	Loss/tok 3.1748 (3.3962)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.183 (0.165)	Data 2.66e-04 (3.34e-04)	Tok/s 92805 (85996)	Loss/tok 3.3610 (3.3956)	LR 2.000e-03
:::MLL 1571748356.802 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1571748356.803 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.646 (0.646)	Decoder iters 113.0 (113.0)	Tok/s 25155 (25155)
0: Running moses detokenizer
0: BLEU(score=22.27505985810351, counts=[36077, 17390, 9635, 5571], totals=[65411, 62408, 59405, 56406], precisions=[55.15433184021036, 27.865017305473657, 16.219173470246613, 9.87660887139666], bp=1.0, sys_len=65411, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571748358.752 eval_accuracy: {"value": 22.28, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1571748358.753 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3970	Test BLEU: 22.28
0: Performance: Epoch: 1	Training: 687833 Tok/s
0: Finished epoch 1
:::MLL 1571748358.753 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1571748358.754 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571748358.754 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3883201836
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][0/1938]	Time 0.528 (0.528)	Data 3.49e-01 (3.49e-01)	Tok/s 31224 (31224)	Loss/tok 3.3166 (3.3166)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.183 (0.184)	Data 2.58e-04 (3.20e-02)	Tok/s 90779 (80646)	Loss/tok 3.2860 (3.1710)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.313 (0.183)	Data 1.70e-04 (1.69e-02)	Tok/s 95208 (83513)	Loss/tok 3.6456 (3.2672)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.182 (0.188)	Data 1.66e-04 (1.15e-02)	Tok/s 91796 (85585)	Loss/tok 3.2618 (3.3134)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.181 (0.182)	Data 4.15e-04 (8.78e-03)	Tok/s 92751 (86158)	Loss/tok 3.2072 (3.2779)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.127 (0.178)	Data 2.37e-04 (7.11e-03)	Tok/s 80538 (86092)	Loss/tok 3.0759 (3.2733)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.126 (0.181)	Data 2.07e-04 (5.97e-03)	Tok/s 82173 (86620)	Loss/tok 3.0493 (3.2828)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.183 (0.181)	Data 2.38e-04 (5.16e-03)	Tok/s 91671 (86941)	Loss/tok 3.1698 (3.2808)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.184 (0.181)	Data 1.67e-04 (4.55e-03)	Tok/s 90578 (87245)	Loss/tok 3.3398 (3.2771)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.127 (0.177)	Data 1.65e-04 (4.07e-03)	Tok/s 80936 (86897)	Loss/tok 3.0233 (3.2655)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.241 (0.176)	Data 2.56e-04 (3.68e-03)	Tok/s 96578 (87029)	Loss/tok 3.3437 (3.2639)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.186 (0.175)	Data 2.19e-04 (3.37e-03)	Tok/s 90437 (86938)	Loss/tok 3.1147 (3.2621)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.126 (0.173)	Data 1.71e-04 (3.11e-03)	Tok/s 81812 (86806)	Loss/tok 3.0961 (3.2609)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.070 (0.173)	Data 1.76e-04 (2.88e-03)	Tok/s 75573 (86873)	Loss/tok 2.5941 (3.2618)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.241 (0.172)	Data 2.04e-04 (2.69e-03)	Tok/s 95679 (86906)	Loss/tok 3.5168 (3.2635)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.128 (0.173)	Data 2.42e-04 (2.53e-03)	Tok/s 82217 (87026)	Loss/tok 2.9935 (3.2701)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.128 (0.173)	Data 1.80e-04 (2.38e-03)	Tok/s 81491 (86839)	Loss/tok 3.1078 (3.2697)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.128 (0.171)	Data 1.69e-04 (2.25e-03)	Tok/s 81104 (86658)	Loss/tok 3.0212 (3.2629)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.184 (0.169)	Data 2.57e-04 (2.14e-03)	Tok/s 92288 (86462)	Loss/tok 3.2968 (3.2582)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.129 (0.168)	Data 1.73e-04 (2.04e-03)	Tok/s 77938 (86402)	Loss/tok 3.0356 (3.2555)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.129 (0.168)	Data 1.69e-04 (1.95e-03)	Tok/s 82116 (86327)	Loss/tok 3.0135 (3.2584)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.241 (0.171)	Data 2.41e-04 (1.86e-03)	Tok/s 95404 (86391)	Loss/tok 3.4476 (3.2738)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.183 (0.169)	Data 2.61e-04 (1.79e-03)	Tok/s 90662 (86167)	Loss/tok 3.2401 (3.2688)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.127 (0.168)	Data 3.24e-04 (1.72e-03)	Tok/s 80666 (86056)	Loss/tok 3.0748 (3.2633)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.126 (0.169)	Data 1.78e-04 (1.66e-03)	Tok/s 83577 (86285)	Loss/tok 3.0154 (3.2698)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.313 (0.168)	Data 2.35e-04 (1.60e-03)	Tok/s 96417 (86108)	Loss/tok 3.5314 (3.2689)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.248 (0.168)	Data 1.71e-04 (1.54e-03)	Tok/s 95495 (86056)	Loss/tok 3.3187 (3.2659)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.185 (0.167)	Data 2.17e-04 (1.49e-03)	Tok/s 90818 (86086)	Loss/tok 3.3167 (3.2639)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.127 (0.168)	Data 1.72e-04 (1.45e-03)	Tok/s 80942 (86190)	Loss/tok 2.9848 (3.2656)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.243 (0.168)	Data 1.66e-04 (1.40e-03)	Tok/s 94325 (86225)	Loss/tok 3.3705 (3.2647)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][300/1938]	Time 0.311 (0.169)	Data 2.06e-04 (1.36e-03)	Tok/s 95122 (86256)	Loss/tok 3.6934 (3.2687)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.128 (0.169)	Data 1.70e-04 (1.33e-03)	Tok/s 82800 (86293)	Loss/tok 3.1030 (3.2672)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.185 (0.169)	Data 3.58e-04 (1.29e-03)	Tok/s 91868 (86363)	Loss/tok 3.2411 (3.2681)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.183 (0.169)	Data 2.59e-04 (1.26e-03)	Tok/s 91491 (86392)	Loss/tok 3.2526 (3.2682)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.128 (0.169)	Data 2.35e-04 (1.23e-03)	Tok/s 80935 (86394)	Loss/tok 3.0689 (3.2667)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.071 (0.169)	Data 2.58e-04 (1.20e-03)	Tok/s 72975 (86401)	Loss/tok 2.5607 (3.2679)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.184 (0.168)	Data 1.89e-04 (1.17e-03)	Tok/s 90684 (86434)	Loss/tok 3.2890 (3.2675)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.130 (0.168)	Data 1.39e-04 (1.15e-03)	Tok/s 80859 (86450)	Loss/tok 3.0806 (3.2678)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.128 (0.168)	Data 1.02e-04 (1.12e-03)	Tok/s 80520 (86388)	Loss/tok 3.0551 (3.2642)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.130 (0.168)	Data 2.56e-04 (1.10e-03)	Tok/s 79424 (86403)	Loss/tok 3.0015 (3.2651)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.240 (0.168)	Data 2.31e-04 (1.07e-03)	Tok/s 98486 (86405)	Loss/tok 3.3189 (3.2647)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.126 (0.167)	Data 1.42e-04 (1.05e-03)	Tok/s 82553 (86324)	Loss/tok 3.0283 (3.2627)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.309 (0.167)	Data 2.58e-04 (1.03e-03)	Tok/s 95242 (86359)	Loss/tok 3.7995 (3.2657)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.072 (0.167)	Data 9.51e-05 (1.01e-03)	Tok/s 72226 (86370)	Loss/tok 2.6299 (3.2635)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.072 (0.167)	Data 1.80e-04 (9.91e-04)	Tok/s 73082 (86338)	Loss/tok 2.6552 (3.2643)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.183 (0.167)	Data 2.04e-04 (9.73e-04)	Tok/s 91513 (86365)	Loss/tok 3.2559 (3.2649)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.125 (0.167)	Data 2.61e-04 (9.55e-04)	Tok/s 82751 (86351)	Loss/tok 3.1403 (3.2666)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][470/1938]	Time 0.128 (0.167)	Data 1.16e-04 (9.38e-04)	Tok/s 79320 (86305)	Loss/tok 3.1285 (3.2672)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.186 (0.168)	Data 1.43e-04 (9.21e-04)	Tok/s 90224 (86369)	Loss/tok 3.3062 (3.2690)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.072 (0.167)	Data 1.39e-04 (9.05e-04)	Tok/s 72755 (86319)	Loss/tok 2.6621 (3.2674)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.128 (0.167)	Data 1.16e-04 (8.90e-04)	Tok/s 79591 (86244)	Loss/tok 2.9893 (3.2649)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.130 (0.167)	Data 1.68e-04 (8.75e-04)	Tok/s 79938 (86269)	Loss/tok 3.0917 (3.2657)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.183 (0.167)	Data 2.19e-04 (8.61e-04)	Tok/s 91976 (86309)	Loss/tok 3.1533 (3.2653)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.126 (0.167)	Data 1.52e-04 (8.47e-04)	Tok/s 80612 (86334)	Loss/tok 2.9147 (3.2640)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.314 (0.167)	Data 2.51e-04 (8.35e-04)	Tok/s 93668 (86391)	Loss/tok 3.6677 (3.2665)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.129 (0.167)	Data 1.70e-04 (8.21e-04)	Tok/s 81513 (86418)	Loss/tok 3.0817 (3.2660)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.239 (0.168)	Data 2.58e-04 (8.10e-04)	Tok/s 97936 (86454)	Loss/tok 3.4045 (3.2673)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.073 (0.167)	Data 1.03e-04 (7.99e-04)	Tok/s 73540 (86380)	Loss/tok 2.7071 (3.2660)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.125 (0.166)	Data 1.52e-04 (7.88e-04)	Tok/s 83536 (86323)	Loss/tok 3.1966 (3.2644)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.076 (0.166)	Data 1.15e-04 (7.77e-04)	Tok/s 68681 (86295)	Loss/tok 2.5891 (3.2628)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.125 (0.166)	Data 1.10e-04 (7.67e-04)	Tok/s 82815 (86247)	Loss/tok 3.1436 (3.2614)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.127 (0.165)	Data 1.17e-04 (7.58e-04)	Tok/s 81282 (86223)	Loss/tok 3.0503 (3.2613)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.124 (0.165)	Data 2.52e-04 (7.49e-04)	Tok/s 83427 (86188)	Loss/tok 3.0020 (3.2595)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.126 (0.165)	Data 2.54e-04 (7.40e-04)	Tok/s 81105 (86217)	Loss/tok 3.0018 (3.2587)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.128 (0.165)	Data 1.22e-04 (7.31e-04)	Tok/s 81525 (86227)	Loss/tok 3.0204 (3.2587)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.184 (0.165)	Data 2.53e-04 (7.23e-04)	Tok/s 91028 (86266)	Loss/tok 3.1848 (3.2594)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][660/1938]	Time 0.128 (0.166)	Data 1.08e-04 (7.14e-04)	Tok/s 79962 (86353)	Loss/tok 3.0597 (3.2636)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.125 (0.166)	Data 1.86e-04 (7.06e-04)	Tok/s 83223 (86304)	Loss/tok 3.0562 (3.2620)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.183 (0.165)	Data 2.00e-04 (6.98e-04)	Tok/s 92024 (86258)	Loss/tok 3.2493 (3.2603)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.240 (0.165)	Data 2.54e-04 (6.90e-04)	Tok/s 98037 (86272)	Loss/tok 3.3142 (3.2607)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.127 (0.165)	Data 1.27e-04 (6.83e-04)	Tok/s 78732 (86242)	Loss/tok 2.9934 (3.2604)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.072 (0.165)	Data 1.13e-04 (6.76e-04)	Tok/s 72774 (86210)	Loss/tok 2.5276 (3.2587)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.242 (0.165)	Data 1.11e-04 (6.69e-04)	Tok/s 94900 (86226)	Loss/tok 3.4688 (3.2595)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.127 (0.165)	Data 1.75e-04 (6.62e-04)	Tok/s 82466 (86183)	Loss/tok 3.1418 (3.2584)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.126 (0.164)	Data 2.65e-04 (6.56e-04)	Tok/s 80841 (86153)	Loss/tok 3.0209 (3.2572)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.126 (0.164)	Data 1.68e-04 (6.50e-04)	Tok/s 82453 (86119)	Loss/tok 3.0228 (3.2558)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.240 (0.164)	Data 1.82e-04 (6.44e-04)	Tok/s 97105 (86143)	Loss/tok 3.5335 (3.2564)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.185 (0.164)	Data 1.08e-04 (6.37e-04)	Tok/s 90035 (86168)	Loss/tok 3.2022 (3.2572)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.185 (0.165)	Data 1.54e-04 (6.31e-04)	Tok/s 91470 (86170)	Loss/tok 3.2976 (3.2581)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][790/1938]	Time 0.183 (0.165)	Data 2.01e-04 (6.25e-04)	Tok/s 90532 (86207)	Loss/tok 3.3680 (3.2604)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.125 (0.165)	Data 1.69e-04 (6.19e-04)	Tok/s 81675 (86228)	Loss/tok 2.9191 (3.2615)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.126 (0.165)	Data 2.37e-04 (6.14e-04)	Tok/s 81468 (86208)	Loss/tok 2.9481 (3.2605)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.125 (0.165)	Data 2.43e-04 (6.10e-04)	Tok/s 83384 (86201)	Loss/tok 3.1073 (3.2603)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.184 (0.165)	Data 9.47e-05 (6.04e-04)	Tok/s 92146 (86219)	Loss/tok 3.3059 (3.2604)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.312 (0.165)	Data 1.10e-04 (5.98e-04)	Tok/s 95011 (86242)	Loss/tok 3.6595 (3.2614)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.185 (0.165)	Data 9.20e-05 (5.92e-04)	Tok/s 91133 (86203)	Loss/tok 3.2588 (3.2607)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.125 (0.165)	Data 9.42e-05 (5.86e-04)	Tok/s 83028 (86245)	Loss/tok 3.0909 (3.2616)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.238 (0.165)	Data 9.66e-05 (5.80e-04)	Tok/s 97873 (86252)	Loss/tok 3.3944 (3.2622)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.128 (0.165)	Data 9.42e-05 (5.75e-04)	Tok/s 81526 (86250)	Loss/tok 3.0125 (3.2619)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.125 (0.165)	Data 9.30e-05 (5.69e-04)	Tok/s 83574 (86222)	Loss/tok 2.9376 (3.2608)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.126 (0.165)	Data 9.42e-05 (5.64e-04)	Tok/s 80876 (86212)	Loss/tok 3.1310 (3.2612)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.125 (0.165)	Data 9.30e-05 (5.59e-04)	Tok/s 81613 (86183)	Loss/tok 3.0846 (3.2610)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][920/1938]	Time 0.309 (0.165)	Data 1.09e-04 (5.54e-04)	Tok/s 95813 (86196)	Loss/tok 3.6917 (3.2639)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.184 (0.165)	Data 9.82e-05 (5.49e-04)	Tok/s 91702 (86205)	Loss/tok 3.3119 (3.2634)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.127 (0.165)	Data 2.51e-04 (5.46e-04)	Tok/s 79381 (86185)	Loss/tok 3.0814 (3.2628)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.181 (0.165)	Data 4.17e-04 (5.42e-04)	Tok/s 92466 (86197)	Loss/tok 3.2777 (3.2633)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.129 (0.165)	Data 1.71e-04 (5.39e-04)	Tok/s 79872 (86195)	Loss/tok 2.9728 (3.2632)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.181 (0.165)	Data 4.16e-04 (5.36e-04)	Tok/s 92146 (86186)	Loss/tok 3.3252 (3.2623)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.182 (0.165)	Data 2.54e-04 (5.33e-04)	Tok/s 91807 (86218)	Loss/tok 3.3090 (3.2633)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.181 (0.165)	Data 3.28e-04 (5.30e-04)	Tok/s 92149 (86213)	Loss/tok 3.1877 (3.2624)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.130 (0.165)	Data 1.69e-04 (5.27e-04)	Tok/s 80967 (86208)	Loss/tok 3.1142 (3.2634)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.180 (0.165)	Data 1.69e-04 (5.25e-04)	Tok/s 93318 (86166)	Loss/tok 3.2233 (3.2622)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.127 (0.164)	Data 2.36e-04 (5.22e-04)	Tok/s 83808 (86135)	Loss/tok 3.1208 (3.2608)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.128 (0.164)	Data 1.23e-04 (5.19e-04)	Tok/s 81337 (86153)	Loss/tok 3.0192 (3.2615)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.070 (0.164)	Data 2.57e-04 (5.17e-04)	Tok/s 75031 (86159)	Loss/tok 2.7052 (3.2612)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.127 (0.164)	Data 4.29e-04 (5.15e-04)	Tok/s 80097 (86148)	Loss/tok 2.9695 (3.2602)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.071 (0.164)	Data 2.71e-04 (5.13e-04)	Tok/s 75170 (86142)	Loss/tok 2.6500 (3.2593)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.184 (0.164)	Data 2.35e-04 (5.10e-04)	Tok/s 90447 (86114)	Loss/tok 3.1975 (3.2581)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.184 (0.164)	Data 2.59e-04 (5.08e-04)	Tok/s 90953 (86097)	Loss/tok 3.1913 (3.2571)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.187 (0.164)	Data 2.20e-04 (5.06e-04)	Tok/s 88675 (86129)	Loss/tok 3.3549 (3.2574)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.069 (0.164)	Data 4.20e-04 (5.04e-04)	Tok/s 76790 (86143)	Loss/tok 2.6534 (3.2583)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.127 (0.164)	Data 4.24e-04 (5.02e-04)	Tok/s 79065 (86130)	Loss/tok 3.0686 (3.2579)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.245 (0.164)	Data 2.71e-04 (5.00e-04)	Tok/s 95208 (86155)	Loss/tok 3.4008 (3.2588)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.068 (0.164)	Data 2.57e-04 (4.97e-04)	Tok/s 76060 (86126)	Loss/tok 2.6454 (3.2579)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.185 (0.164)	Data 1.77e-04 (4.95e-04)	Tok/s 90655 (86107)	Loss/tok 3.2020 (3.2571)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.130 (0.164)	Data 1.39e-04 (4.92e-04)	Tok/s 79522 (86118)	Loss/tok 3.0801 (3.2580)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.186 (0.164)	Data 2.73e-04 (4.90e-04)	Tok/s 90447 (86148)	Loss/tok 3.1201 (3.2584)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.182 (0.164)	Data 1.74e-04 (4.88e-04)	Tok/s 92249 (86120)	Loss/tok 3.3149 (3.2573)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.125 (0.164)	Data 1.68e-04 (4.86e-04)	Tok/s 82953 (86131)	Loss/tok 3.0308 (3.2585)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.132 (0.164)	Data 1.28e-04 (4.84e-04)	Tok/s 79410 (86151)	Loss/tok 3.0617 (3.2585)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.126 (0.164)	Data 2.71e-04 (4.82e-04)	Tok/s 81023 (86157)	Loss/tok 3.0377 (3.2587)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.131 (0.164)	Data 2.81e-04 (4.80e-04)	Tok/s 78560 (86139)	Loss/tok 3.1633 (3.2582)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.186 (0.164)	Data 4.01e-04 (4.79e-04)	Tok/s 91446 (86142)	Loss/tok 3.2769 (3.2584)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.130 (0.164)	Data 3.31e-04 (4.77e-04)	Tok/s 80582 (86143)	Loss/tok 3.0897 (3.2583)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.128 (0.164)	Data 2.89e-04 (4.75e-04)	Tok/s 82367 (86160)	Loss/tok 3.0204 (3.2585)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.129 (0.164)	Data 2.16e-04 (4.73e-04)	Tok/s 78941 (86167)	Loss/tok 2.9805 (3.2590)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.130 (0.164)	Data 2.23e-04 (4.71e-04)	Tok/s 77401 (86169)	Loss/tok 3.0077 (3.2592)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1270/1938]	Time 0.127 (0.165)	Data 2.54e-04 (4.69e-04)	Tok/s 81505 (86182)	Loss/tok 3.0573 (3.2598)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.126 (0.165)	Data 2.62e-04 (4.67e-04)	Tok/s 82077 (86222)	Loss/tok 3.0357 (3.2612)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.128 (0.165)	Data 3.96e-04 (4.65e-04)	Tok/s 80388 (86201)	Loss/tok 3.1225 (3.2612)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1300/1938]	Time 0.239 (0.165)	Data 2.62e-04 (4.64e-04)	Tok/s 96065 (86224)	Loss/tok 3.5790 (3.2622)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.186 (0.165)	Data 1.85e-04 (4.62e-04)	Tok/s 90822 (86251)	Loss/tok 3.2408 (3.2632)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.126 (0.165)	Data 2.55e-04 (4.60e-04)	Tok/s 82161 (86245)	Loss/tok 3.0657 (3.2626)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.312 (0.165)	Data 1.69e-04 (4.59e-04)	Tok/s 95052 (86235)	Loss/tok 3.6364 (3.2622)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.315 (0.165)	Data 1.75e-04 (4.57e-04)	Tok/s 93412 (86269)	Loss/tok 3.7261 (3.2644)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.184 (0.165)	Data 2.74e-04 (4.55e-04)	Tok/s 91066 (86277)	Loss/tok 3.2226 (3.2641)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.070 (0.165)	Data 2.70e-04 (4.54e-04)	Tok/s 73889 (86275)	Loss/tok 2.5947 (3.2636)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.131 (0.165)	Data 1.70e-04 (4.52e-04)	Tok/s 79944 (86264)	Loss/tok 3.0979 (3.2634)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.129 (0.165)	Data 1.70e-04 (4.51e-04)	Tok/s 79695 (86252)	Loss/tok 3.0815 (3.2629)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.133 (0.165)	Data 2.54e-04 (4.50e-04)	Tok/s 78074 (86256)	Loss/tok 3.0694 (3.2626)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.241 (0.165)	Data 1.49e-04 (4.48e-04)	Tok/s 97076 (86246)	Loss/tok 3.4949 (3.2623)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.240 (0.165)	Data 2.53e-04 (4.46e-04)	Tok/s 96563 (86242)	Loss/tok 3.3540 (3.2617)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.185 (0.165)	Data 9.85e-05 (4.44e-04)	Tok/s 91414 (86258)	Loss/tok 3.1770 (3.2621)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.131 (0.165)	Data 2.55e-04 (4.42e-04)	Tok/s 78925 (86229)	Loss/tok 3.0283 (3.2611)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.184 (0.165)	Data 1.20e-04 (4.41e-04)	Tok/s 92448 (86232)	Loss/tok 3.1694 (3.2608)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.127 (0.165)	Data 1.56e-04 (4.39e-04)	Tok/s 82870 (86216)	Loss/tok 3.0041 (3.2598)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.186 (0.165)	Data 1.89e-04 (4.37e-04)	Tok/s 89700 (86218)	Loss/tok 3.3543 (3.2598)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.127 (0.165)	Data 2.00e-04 (4.35e-04)	Tok/s 80841 (86232)	Loss/tok 3.0179 (3.2606)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.184 (0.165)	Data 2.16e-04 (4.33e-04)	Tok/s 91220 (86240)	Loss/tok 3.2589 (3.2605)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.186 (0.165)	Data 1.91e-04 (4.32e-04)	Tok/s 90483 (86244)	Loss/tok 3.2344 (3.2605)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.242 (0.165)	Data 2.31e-04 (4.30e-04)	Tok/s 95721 (86264)	Loss/tok 3.5553 (3.2611)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.071 (0.165)	Data 1.67e-04 (4.28e-04)	Tok/s 72947 (86240)	Loss/tok 2.6286 (3.2603)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.127 (0.165)	Data 1.51e-04 (4.27e-04)	Tok/s 81597 (86216)	Loss/tok 2.9631 (3.2597)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.074 (0.165)	Data 1.11e-04 (4.25e-04)	Tok/s 71902 (86192)	Loss/tok 2.6619 (3.2589)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.072 (0.165)	Data 9.75e-05 (4.23e-04)	Tok/s 73360 (86197)	Loss/tok 2.6126 (3.2595)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.244 (0.165)	Data 1.13e-04 (4.21e-04)	Tok/s 94569 (86217)	Loss/tok 3.5350 (3.2595)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.129 (0.165)	Data 1.42e-04 (4.19e-04)	Tok/s 79351 (86215)	Loss/tok 3.0542 (3.2597)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.244 (0.165)	Data 9.18e-05 (4.18e-04)	Tok/s 96406 (86226)	Loss/tok 3.3468 (3.2595)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.313 (0.165)	Data 1.71e-04 (4.16e-04)	Tok/s 94788 (86215)	Loss/tok 3.5436 (3.2593)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.309 (0.165)	Data 1.67e-04 (4.14e-04)	Tok/s 94413 (86200)	Loss/tok 3.6332 (3.2596)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.185 (0.165)	Data 1.67e-04 (4.13e-04)	Tok/s 90124 (86197)	Loss/tok 3.2175 (3.2594)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.126 (0.165)	Data 2.35e-04 (4.11e-04)	Tok/s 82094 (86189)	Loss/tok 3.0943 (3.2591)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.241 (0.165)	Data 1.53e-04 (4.10e-04)	Tok/s 96359 (86192)	Loss/tok 3.4219 (3.2592)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.183 (0.165)	Data 2.53e-04 (4.08e-04)	Tok/s 92125 (86184)	Loss/tok 3.3432 (3.2588)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.128 (0.165)	Data 1.84e-04 (4.07e-04)	Tok/s 80439 (86175)	Loss/tok 3.0682 (3.2584)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.129 (0.165)	Data 2.67e-04 (4.06e-04)	Tok/s 81244 (86173)	Loss/tok 3.1209 (3.2581)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.182 (0.165)	Data 1.26e-04 (4.05e-04)	Tok/s 91115 (86202)	Loss/tok 3.3029 (3.2593)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.184 (0.165)	Data 1.78e-04 (4.03e-04)	Tok/s 92950 (86211)	Loss/tok 3.2939 (3.2602)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.183 (0.165)	Data 2.20e-04 (4.02e-04)	Tok/s 91416 (86204)	Loss/tok 3.1591 (3.2602)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1690/1938]	Time 0.184 (0.165)	Data 1.90e-04 (4.01e-04)	Tok/s 91138 (86224)	Loss/tok 3.2645 (3.2608)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.182 (0.165)	Data 2.70e-04 (4.00e-04)	Tok/s 91990 (86195)	Loss/tok 3.2939 (3.2600)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.130 (0.165)	Data 1.27e-04 (3.99e-04)	Tok/s 79786 (86230)	Loss/tok 3.0527 (3.2616)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.128 (0.165)	Data 1.98e-04 (3.98e-04)	Tok/s 80495 (86235)	Loss/tok 2.9925 (3.2615)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.130 (0.165)	Data 1.76e-04 (3.96e-04)	Tok/s 78785 (86220)	Loss/tok 3.1388 (3.2612)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.242 (0.165)	Data 1.79e-04 (3.95e-04)	Tok/s 95341 (86224)	Loss/tok 3.5403 (3.2612)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.127 (0.165)	Data 1.77e-04 (3.94e-04)	Tok/s 80995 (86212)	Loss/tok 3.0442 (3.2610)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.130 (0.165)	Data 2.26e-04 (3.93e-04)	Tok/s 79743 (86202)	Loss/tok 3.0157 (3.2609)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.134 (0.165)	Data 1.69e-04 (3.92e-04)	Tok/s 76004 (86193)	Loss/tok 2.9004 (3.2607)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.244 (0.166)	Data 1.76e-04 (3.91e-04)	Tok/s 96117 (86216)	Loss/tok 3.4428 (3.2614)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.129 (0.166)	Data 1.75e-04 (3.89e-04)	Tok/s 80913 (86224)	Loss/tok 3.1801 (3.2615)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.184 (0.166)	Data 1.75e-04 (3.88e-04)	Tok/s 90614 (86198)	Loss/tok 3.2686 (3.2609)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.247 (0.166)	Data 1.75e-04 (3.87e-04)	Tok/s 93493 (86197)	Loss/tok 3.4177 (3.2610)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.129 (0.165)	Data 2.39e-04 (3.86e-04)	Tok/s 81911 (86173)	Loss/tok 3.1592 (3.2609)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.129 (0.165)	Data 1.70e-04 (3.85e-04)	Tok/s 79248 (86159)	Loss/tok 3.1200 (3.2605)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.132 (0.165)	Data 2.19e-04 (3.84e-04)	Tok/s 77023 (86142)	Loss/tok 2.9357 (3.2600)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.132 (0.165)	Data 1.22e-04 (3.83e-04)	Tok/s 78465 (86141)	Loss/tok 3.0878 (3.2600)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.127 (0.165)	Data 1.73e-04 (3.82e-04)	Tok/s 81173 (86137)	Loss/tok 2.9539 (3.2597)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.241 (0.165)	Data 1.71e-04 (3.81e-04)	Tok/s 96582 (86123)	Loss/tok 3.3860 (3.2595)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.133 (0.165)	Data 1.25e-04 (3.80e-04)	Tok/s 77238 (86121)	Loss/tok 3.0256 (3.2601)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.315 (0.165)	Data 1.71e-04 (3.80e-04)	Tok/s 94509 (86125)	Loss/tok 3.5883 (3.2602)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.129 (0.165)	Data 1.73e-04 (3.79e-04)	Tok/s 80401 (86119)	Loss/tok 2.9502 (3.2600)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.313 (0.165)	Data 1.71e-04 (3.78e-04)	Tok/s 95362 (86111)	Loss/tok 3.5067 (3.2602)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.240 (0.165)	Data 2.01e-04 (3.77e-04)	Tok/s 97786 (86107)	Loss/tok 3.3488 (3.2597)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.129 (0.165)	Data 1.70e-04 (3.76e-04)	Tok/s 78218 (86108)	Loss/tok 3.1008 (3.2598)	LR 2.000e-03
:::MLL 1571748680.006 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1571748680.007 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.605 (0.605)	Decoder iters 99.0 (99.0)	Tok/s 26669 (26669)
0: Running moses detokenizer
0: BLEU(score=23.106489169169294, counts=[36138, 17726, 9942, 5817], totals=[64138, 61135, 58132, 55134], precisions=[56.34413296329789, 28.994847468716774, 17.102456478359596, 10.550658395908151], bp=0.991646919454928, sys_len=64138, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571748681.846 eval_accuracy: {"value": 23.11, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1571748681.846 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.2585	Test BLEU: 23.11
0: Performance: Epoch: 2	Training: 688759 Tok/s
0: Finished epoch 2
:::MLL 1571748681.847 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1571748681.847 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571748681.848 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3863754983
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][0/1938]	Time 0.576 (0.576)	Data 3.05e-01 (3.05e-01)	Tok/s 29282 (29282)	Loss/tok 3.0298 (3.0298)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.187 (0.185)	Data 1.71e-04 (2.79e-02)	Tok/s 90175 (78587)	Loss/tok 3.1440 (3.0219)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.127 (0.166)	Data 9.70e-05 (1.47e-02)	Tok/s 79691 (80926)	Loss/tok 3.0052 (3.0340)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.127 (0.165)	Data 9.32e-05 (9.97e-03)	Tok/s 80887 (82395)	Loss/tok 2.8719 (3.0657)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.128 (0.165)	Data 9.70e-05 (7.56e-03)	Tok/s 79720 (82629)	Loss/tok 2.8750 (3.0990)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.129 (0.164)	Data 9.89e-05 (6.10e-03)	Tok/s 79328 (82982)	Loss/tok 2.8904 (3.1060)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.312 (0.164)	Data 9.44e-05 (5.12e-03)	Tok/s 96225 (83295)	Loss/tok 3.4376 (3.1111)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.187 (0.167)	Data 9.54e-05 (4.41e-03)	Tok/s 89121 (83910)	Loss/tok 3.1397 (3.1220)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.127 (0.169)	Data 9.68e-05 (3.88e-03)	Tok/s 81942 (84381)	Loss/tok 2.9679 (3.1267)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.186 (0.166)	Data 9.63e-05 (3.46e-03)	Tok/s 90014 (84308)	Loss/tok 3.2505 (3.1223)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.127 (0.167)	Data 9.82e-05 (3.13e-03)	Tok/s 81081 (84470)	Loss/tok 2.9105 (3.1291)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.132 (0.166)	Data 9.73e-05 (2.86e-03)	Tok/s 80881 (84599)	Loss/tok 2.9684 (3.1261)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.186 (0.167)	Data 1.09e-04 (2.63e-03)	Tok/s 91064 (84994)	Loss/tok 2.9833 (3.1295)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.129 (0.165)	Data 9.39e-05 (2.43e-03)	Tok/s 79442 (84776)	Loss/tok 2.9752 (3.1263)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.312 (0.167)	Data 9.78e-05 (2.27e-03)	Tok/s 96339 (84930)	Loss/tok 3.4184 (3.1342)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.244 (0.167)	Data 9.44e-05 (2.12e-03)	Tok/s 96281 (85021)	Loss/tok 3.3113 (3.1351)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][160/1938]	Time 0.133 (0.167)	Data 9.47e-05 (2.00e-03)	Tok/s 77940 (85006)	Loss/tok 3.0355 (3.1418)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.183 (0.168)	Data 9.56e-05 (1.89e-03)	Tok/s 92376 (85050)	Loss/tok 3.1542 (3.1467)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.126 (0.169)	Data 9.58e-05 (1.79e-03)	Tok/s 83832 (85372)	Loss/tok 3.0011 (3.1552)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.184 (0.169)	Data 9.54e-05 (1.70e-03)	Tok/s 91552 (85351)	Loss/tok 3.1774 (3.1540)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.241 (0.169)	Data 1.10e-04 (1.62e-03)	Tok/s 95441 (85471)	Loss/tok 3.3345 (3.1557)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.184 (0.170)	Data 9.80e-05 (1.55e-03)	Tok/s 89910 (85590)	Loss/tok 3.2360 (3.1604)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.182 (0.170)	Data 9.23e-05 (1.48e-03)	Tok/s 90718 (85624)	Loss/tok 3.2610 (3.1603)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.128 (0.169)	Data 9.37e-05 (1.42e-03)	Tok/s 80037 (85628)	Loss/tok 2.9805 (3.1601)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.186 (0.170)	Data 9.63e-05 (1.37e-03)	Tok/s 89107 (85733)	Loss/tok 3.1762 (3.1597)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.312 (0.171)	Data 9.54e-05 (1.32e-03)	Tok/s 94361 (85848)	Loss/tok 3.5608 (3.1644)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.185 (0.170)	Data 9.37e-05 (1.27e-03)	Tok/s 90676 (85767)	Loss/tok 3.2492 (3.1614)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.186 (0.169)	Data 9.92e-05 (1.23e-03)	Tok/s 88692 (85739)	Loss/tok 3.1505 (3.1597)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.127 (0.169)	Data 9.63e-05 (1.19e-03)	Tok/s 82150 (85774)	Loss/tok 3.0838 (3.1612)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.185 (0.169)	Data 9.66e-05 (1.15e-03)	Tok/s 91282 (85738)	Loss/tok 3.1302 (3.1603)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][300/1938]	Time 0.132 (0.169)	Data 9.30e-05 (1.12e-03)	Tok/s 80037 (85771)	Loss/tok 3.0088 (3.1621)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.127 (0.168)	Data 9.61e-05 (1.08e-03)	Tok/s 80347 (85670)	Loss/tok 3.0848 (3.1598)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.241 (0.168)	Data 9.54e-05 (1.05e-03)	Tok/s 97071 (85683)	Loss/tok 3.3623 (3.1601)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.131 (0.167)	Data 1.48e-04 (1.02e-03)	Tok/s 79526 (85598)	Loss/tok 3.1175 (3.1597)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.188 (0.167)	Data 9.44e-05 (9.96e-04)	Tok/s 88442 (85560)	Loss/tok 3.1727 (3.1576)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.131 (0.168)	Data 1.03e-04 (9.71e-04)	Tok/s 77542 (85656)	Loss/tok 3.0071 (3.1614)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.073 (0.168)	Data 9.75e-05 (9.47e-04)	Tok/s 71526 (85683)	Loss/tok 2.5236 (3.1625)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.189 (0.168)	Data 9.30e-05 (9.24e-04)	Tok/s 86894 (85738)	Loss/tok 3.1810 (3.1631)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.243 (0.169)	Data 9.89e-05 (9.02e-04)	Tok/s 96273 (85838)	Loss/tok 3.2728 (3.1688)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.184 (0.169)	Data 9.78e-05 (8.82e-04)	Tok/s 93709 (85909)	Loss/tok 3.1804 (3.1703)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.068 (0.169)	Data 9.73e-05 (8.62e-04)	Tok/s 79630 (85924)	Loss/tok 2.6265 (3.1697)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.126 (0.169)	Data 9.58e-05 (8.43e-04)	Tok/s 80994 (85942)	Loss/tok 3.0684 (3.1715)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.069 (0.169)	Data 9.42e-05 (8.26e-04)	Tok/s 75810 (85886)	Loss/tok 2.5655 (3.1699)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.131 (0.168)	Data 9.54e-05 (8.09e-04)	Tok/s 80854 (85788)	Loss/tok 2.9878 (3.1685)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.242 (0.169)	Data 1.11e-04 (7.93e-04)	Tok/s 95915 (85877)	Loss/tok 3.4472 (3.1711)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.127 (0.168)	Data 1.03e-04 (7.77e-04)	Tok/s 83617 (85893)	Loss/tok 2.9357 (3.1707)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.070 (0.168)	Data 1.01e-04 (7.63e-04)	Tok/s 74256 (85851)	Loss/tok 2.6080 (3.1698)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.189 (0.168)	Data 9.56e-05 (7.49e-04)	Tok/s 87035 (85853)	Loss/tok 3.2768 (3.1688)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.186 (0.167)	Data 9.61e-05 (7.35e-04)	Tok/s 89970 (85813)	Loss/tok 3.1059 (3.1673)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.436 (0.167)	Data 9.80e-05 (7.22e-04)	Tok/s 68533 (85762)	Loss/tok 3.4888 (3.1692)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][500/1938]	Time 0.242 (0.167)	Data 1.02e-04 (7.10e-04)	Tok/s 97248 (85717)	Loss/tok 3.1694 (3.1683)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.132 (0.168)	Data 9.78e-05 (6.98e-04)	Tok/s 78116 (85738)	Loss/tok 3.1086 (3.1720)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.238 (0.168)	Data 9.37e-05 (6.86e-04)	Tok/s 98657 (85727)	Loss/tok 3.2532 (3.1726)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.127 (0.167)	Data 9.32e-05 (6.75e-04)	Tok/s 81165 (85664)	Loss/tok 3.0766 (3.1718)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.186 (0.167)	Data 9.99e-05 (6.64e-04)	Tok/s 90545 (85659)	Loss/tok 3.2012 (3.1729)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.126 (0.167)	Data 9.49e-05 (6.54e-04)	Tok/s 84219 (85670)	Loss/tok 2.9892 (3.1737)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.128 (0.167)	Data 9.37e-05 (6.44e-04)	Tok/s 80322 (85633)	Loss/tok 2.8893 (3.1725)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.127 (0.167)	Data 9.70e-05 (6.34e-04)	Tok/s 81212 (85619)	Loss/tok 3.2069 (3.1720)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.243 (0.167)	Data 9.32e-05 (6.25e-04)	Tok/s 96697 (85606)	Loss/tok 3.3052 (3.1715)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.128 (0.166)	Data 9.49e-05 (6.16e-04)	Tok/s 81233 (85521)	Loss/tok 3.0028 (3.1696)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.186 (0.166)	Data 9.66e-05 (6.08e-04)	Tok/s 90436 (85551)	Loss/tok 3.2392 (3.1697)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.242 (0.166)	Data 9.75e-05 (5.99e-04)	Tok/s 96442 (85601)	Loss/tok 3.2884 (3.1724)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.243 (0.166)	Data 9.70e-05 (5.91e-04)	Tok/s 95266 (85568)	Loss/tok 3.3707 (3.1714)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.126 (0.166)	Data 9.27e-05 (5.83e-04)	Tok/s 82047 (85498)	Loss/tok 2.9736 (3.1702)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.129 (0.165)	Data 2.62e-04 (5.77e-04)	Tok/s 81896 (85477)	Loss/tok 2.9769 (3.1696)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.186 (0.165)	Data 2.04e-04 (5.71e-04)	Tok/s 89534 (85488)	Loss/tok 3.2029 (3.1691)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.183 (0.165)	Data 1.66e-04 (5.65e-04)	Tok/s 92451 (85518)	Loss/tok 3.2217 (3.1696)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.128 (0.165)	Data 2.28e-04 (5.59e-04)	Tok/s 80771 (85507)	Loss/tok 3.0668 (3.1697)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.183 (0.165)	Data 1.06e-04 (5.53e-04)	Tok/s 91724 (85503)	Loss/tok 3.1547 (3.1692)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.129 (0.165)	Data 1.08e-04 (5.47e-04)	Tok/s 79872 (85519)	Loss/tok 2.9175 (3.1696)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.127 (0.165)	Data 2.18e-04 (5.42e-04)	Tok/s 82147 (85466)	Loss/tok 2.9632 (3.1683)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.243 (0.165)	Data 1.51e-04 (5.36e-04)	Tok/s 96511 (85515)	Loss/tok 3.2921 (3.1699)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.245 (0.165)	Data 1.10e-04 (5.31e-04)	Tok/s 93895 (85526)	Loss/tok 3.3119 (3.1698)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.127 (0.165)	Data 9.47e-05 (5.26e-04)	Tok/s 81927 (85509)	Loss/tok 2.9137 (3.1685)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.245 (0.165)	Data 2.51e-04 (5.22e-04)	Tok/s 94914 (85573)	Loss/tok 3.4404 (3.1694)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.309 (0.165)	Data 1.62e-04 (5.17e-04)	Tok/s 96945 (85562)	Loss/tok 3.4795 (3.1689)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.070 (0.165)	Data 1.40e-04 (5.12e-04)	Tok/s 76483 (85579)	Loss/tok 2.5876 (3.1695)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.127 (0.165)	Data 2.16e-04 (5.07e-04)	Tok/s 80804 (85563)	Loss/tok 2.9994 (3.1687)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.127 (0.165)	Data 1.42e-04 (5.02e-04)	Tok/s 82900 (85580)	Loss/tok 2.8585 (3.1688)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.129 (0.165)	Data 2.04e-04 (4.98e-04)	Tok/s 80825 (85556)	Loss/tok 2.9462 (3.1687)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.126 (0.165)	Data 1.60e-04 (4.94e-04)	Tok/s 81326 (85534)	Loss/tok 3.0295 (3.1678)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.184 (0.165)	Data 1.09e-04 (4.90e-04)	Tok/s 91872 (85587)	Loss/tok 3.1494 (3.1689)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.125 (0.165)	Data 1.61e-04 (4.86e-04)	Tok/s 83245 (85560)	Loss/tok 2.9675 (3.1679)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.241 (0.165)	Data 1.09e-04 (4.82e-04)	Tok/s 96118 (85582)	Loss/tok 3.3429 (3.1684)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.075 (0.165)	Data 1.04e-04 (4.78e-04)	Tok/s 70945 (85560)	Loss/tok 2.5937 (3.1674)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.241 (0.165)	Data 1.08e-04 (4.74e-04)	Tok/s 97292 (85603)	Loss/tok 3.3610 (3.1682)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.128 (0.165)	Data 1.37e-04 (4.70e-04)	Tok/s 81813 (85593)	Loss/tok 2.9990 (3.1675)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.243 (0.165)	Data 1.14e-04 (4.67e-04)	Tok/s 96044 (85609)	Loss/tok 3.2360 (3.1667)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.128 (0.165)	Data 1.52e-04 (4.63e-04)	Tok/s 82381 (85619)	Loss/tok 2.9620 (3.1664)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.315 (0.165)	Data 2.55e-04 (4.60e-04)	Tok/s 93880 (85648)	Loss/tok 3.6609 (3.1671)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.183 (0.165)	Data 1.32e-04 (4.56e-04)	Tok/s 91162 (85639)	Loss/tok 3.1053 (3.1670)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.130 (0.165)	Data 1.41e-04 (4.53e-04)	Tok/s 78413 (85652)	Loss/tok 2.8374 (3.1673)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.185 (0.165)	Data 1.53e-04 (4.50e-04)	Tok/s 90669 (85662)	Loss/tok 3.0525 (3.1665)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.185 (0.166)	Data 1.14e-04 (4.47e-04)	Tok/s 90258 (85710)	Loss/tok 3.2253 (3.1679)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.246 (0.166)	Data 1.07e-04 (4.44e-04)	Tok/s 93543 (85745)	Loss/tok 3.4029 (3.1686)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.127 (0.166)	Data 2.37e-04 (4.41e-04)	Tok/s 83290 (85718)	Loss/tok 3.0865 (3.1679)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.185 (0.166)	Data 1.78e-04 (4.38e-04)	Tok/s 90983 (85717)	Loss/tok 3.1590 (3.1679)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.133 (0.165)	Data 1.06e-04 (4.35e-04)	Tok/s 77965 (85680)	Loss/tok 2.8783 (3.1666)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.245 (0.166)	Data 9.78e-05 (4.32e-04)	Tok/s 96256 (85712)	Loss/tok 3.1482 (3.1675)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.127 (0.165)	Data 2.67e-04 (4.29e-04)	Tok/s 82281 (85686)	Loss/tok 3.0673 (3.1663)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.125 (0.165)	Data 1.46e-04 (4.26e-04)	Tok/s 82579 (85679)	Loss/tok 2.9475 (3.1661)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.189 (0.165)	Data 1.12e-04 (4.23e-04)	Tok/s 88598 (85679)	Loss/tok 3.1333 (3.1652)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1020/1938]	Time 0.129 (0.165)	Data 2.54e-04 (4.20e-04)	Tok/s 81782 (85692)	Loss/tok 2.9844 (3.1656)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.131 (0.166)	Data 9.85e-05 (4.18e-04)	Tok/s 78110 (85706)	Loss/tok 2.9723 (3.1651)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.128 (0.165)	Data 9.66e-05 (4.15e-04)	Tok/s 80868 (85708)	Loss/tok 3.0115 (3.1647)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.188 (0.165)	Data 1.12e-04 (4.12e-04)	Tok/s 90081 (85711)	Loss/tok 3.0713 (3.1641)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.069 (0.166)	Data 1.07e-04 (4.09e-04)	Tok/s 79241 (85749)	Loss/tok 2.4938 (3.1638)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.127 (0.165)	Data 1.49e-04 (4.07e-04)	Tok/s 81112 (85727)	Loss/tok 2.8800 (3.1624)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.131 (0.165)	Data 2.55e-04 (4.05e-04)	Tok/s 80813 (85709)	Loss/tok 2.8933 (3.1613)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.129 (0.165)	Data 2.60e-04 (4.03e-04)	Tok/s 80405 (85682)	Loss/tok 3.0366 (3.1602)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.129 (0.165)	Data 1.66e-04 (4.02e-04)	Tok/s 79792 (85662)	Loss/tok 2.9320 (3.1590)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.186 (0.165)	Data 1.70e-04 (4.00e-04)	Tok/s 89959 (85694)	Loss/tok 3.2032 (3.1591)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.189 (0.165)	Data 1.68e-04 (3.98e-04)	Tok/s 90102 (85699)	Loss/tok 3.1147 (3.1589)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.127 (0.164)	Data 1.81e-04 (3.96e-04)	Tok/s 80356 (85663)	Loss/tok 2.9326 (3.1578)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1140/1938]	Time 0.128 (0.165)	Data 2.35e-04 (3.94e-04)	Tok/s 80553 (85678)	Loss/tok 2.9081 (3.1590)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.126 (0.165)	Data 1.67e-04 (3.92e-04)	Tok/s 82010 (85658)	Loss/tok 3.0103 (3.1583)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.131 (0.165)	Data 2.51e-04 (3.90e-04)	Tok/s 80533 (85689)	Loss/tok 2.9400 (3.1593)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.068 (0.165)	Data 1.67e-04 (3.88e-04)	Tok/s 78551 (85703)	Loss/tok 2.4796 (3.1588)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.187 (0.165)	Data 9.18e-05 (3.86e-04)	Tok/s 89593 (85724)	Loss/tok 3.1191 (3.1590)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.183 (0.165)	Data 1.69e-04 (3.85e-04)	Tok/s 91180 (85693)	Loss/tok 3.1118 (3.1579)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.188 (0.165)	Data 9.99e-05 (3.83e-04)	Tok/s 88670 (85697)	Loss/tok 3.2083 (3.1580)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.312 (0.165)	Data 2.12e-04 (3.81e-04)	Tok/s 95252 (85698)	Loss/tok 3.4933 (3.1581)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1220/1938]	Time 0.072 (0.165)	Data 1.03e-04 (3.79e-04)	Tok/s 72166 (85676)	Loss/tok 2.4345 (3.1580)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.127 (0.165)	Data 1.27e-04 (3.77e-04)	Tok/s 83192 (85652)	Loss/tok 2.9687 (3.1572)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.071 (0.165)	Data 2.40e-04 (3.76e-04)	Tok/s 74752 (85674)	Loss/tok 2.5605 (3.1584)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.127 (0.165)	Data 2.53e-04 (3.74e-04)	Tok/s 82420 (85667)	Loss/tok 3.0318 (3.1577)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.127 (0.165)	Data 1.77e-04 (3.73e-04)	Tok/s 79557 (85668)	Loss/tok 3.0139 (3.1572)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.312 (0.165)	Data 1.68e-04 (3.71e-04)	Tok/s 96826 (85673)	Loss/tok 3.5493 (3.1575)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.185 (0.165)	Data 2.57e-04 (3.70e-04)	Tok/s 91318 (85690)	Loss/tok 3.1540 (3.1574)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.126 (0.165)	Data 2.51e-04 (3.68e-04)	Tok/s 80732 (85676)	Loss/tok 3.0252 (3.1574)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.186 (0.165)	Data 2.49e-04 (3.67e-04)	Tok/s 90902 (85702)	Loss/tok 3.0854 (3.1570)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.129 (0.165)	Data 1.72e-04 (3.65e-04)	Tok/s 79929 (85710)	Loss/tok 2.8950 (3.1571)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.128 (0.165)	Data 1.67e-04 (3.64e-04)	Tok/s 79715 (85717)	Loss/tok 2.8524 (3.1577)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.130 (0.165)	Data 1.67e-04 (3.63e-04)	Tok/s 79285 (85738)	Loss/tok 2.9532 (3.1581)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.241 (0.165)	Data 1.67e-04 (3.61e-04)	Tok/s 97494 (85739)	Loss/tok 3.2559 (3.1583)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.128 (0.165)	Data 1.72e-04 (3.60e-04)	Tok/s 79437 (85766)	Loss/tok 3.0033 (3.1590)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.184 (0.165)	Data 2.00e-04 (3.58e-04)	Tok/s 91017 (85755)	Loss/tok 3.1492 (3.1588)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.183 (0.165)	Data 1.43e-04 (3.57e-04)	Tok/s 91853 (85757)	Loss/tok 3.1146 (3.1596)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.074 (0.165)	Data 2.31e-04 (3.56e-04)	Tok/s 71836 (85737)	Loss/tok 2.5081 (3.1591)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.131 (0.165)	Data 1.47e-04 (3.55e-04)	Tok/s 76939 (85692)	Loss/tok 2.9595 (3.1581)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.128 (0.165)	Data 1.25e-04 (3.53e-04)	Tok/s 81133 (85712)	Loss/tok 2.9412 (3.1575)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.185 (0.165)	Data 1.50e-04 (3.52e-04)	Tok/s 89767 (85742)	Loss/tok 3.0966 (3.1581)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.131 (0.165)	Data 2.58e-04 (3.51e-04)	Tok/s 78541 (85734)	Loss/tok 2.9495 (3.1582)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.242 (0.165)	Data 1.71e-04 (3.49e-04)	Tok/s 96286 (85746)	Loss/tok 3.3034 (3.1585)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.185 (0.166)	Data 1.40e-04 (3.48e-04)	Tok/s 90643 (85770)	Loss/tok 3.0003 (3.1591)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.126 (0.165)	Data 1.51e-04 (3.46e-04)	Tok/s 81697 (85747)	Loss/tok 2.8705 (3.1586)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.241 (0.166)	Data 1.39e-04 (3.45e-04)	Tok/s 96710 (85760)	Loss/tok 3.3269 (3.1587)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.312 (0.166)	Data 1.21e-04 (3.44e-04)	Tok/s 95323 (85750)	Loss/tok 3.5194 (3.1586)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.127 (0.165)	Data 1.99e-04 (3.43e-04)	Tok/s 81408 (85746)	Loss/tok 3.0852 (3.1583)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.130 (0.165)	Data 1.29e-04 (3.42e-04)	Tok/s 79443 (85734)	Loss/tok 2.9937 (3.1575)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.314 (0.165)	Data 1.69e-04 (3.40e-04)	Tok/s 93547 (85727)	Loss/tok 3.5448 (3.1578)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1510/1938]	Time 0.128 (0.165)	Data 1.70e-04 (3.39e-04)	Tok/s 79849 (85732)	Loss/tok 3.0412 (3.1578)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.187 (0.165)	Data 1.66e-04 (3.38e-04)	Tok/s 89300 (85743)	Loss/tok 3.1380 (3.1580)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.129 (0.166)	Data 2.61e-04 (3.37e-04)	Tok/s 80457 (85760)	Loss/tok 2.9505 (3.1580)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.127 (0.166)	Data 1.77e-04 (3.36e-04)	Tok/s 79049 (85758)	Loss/tok 2.9297 (3.1574)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.075 (0.165)	Data 1.46e-04 (3.34e-04)	Tok/s 69339 (85749)	Loss/tok 2.5697 (3.1568)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.073 (0.165)	Data 1.26e-04 (3.33e-04)	Tok/s 70468 (85745)	Loss/tok 2.4946 (3.1566)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.129 (0.165)	Data 1.52e-04 (3.32e-04)	Tok/s 80217 (85747)	Loss/tok 2.8457 (3.1567)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.131 (0.166)	Data 1.29e-04 (3.31e-04)	Tok/s 79472 (85761)	Loss/tok 2.9264 (3.1566)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.247 (0.166)	Data 1.30e-04 (3.30e-04)	Tok/s 93245 (85782)	Loss/tok 3.2275 (3.1563)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.182 (0.166)	Data 2.17e-04 (3.29e-04)	Tok/s 93467 (85761)	Loss/tok 3.1510 (3.1559)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.127 (0.166)	Data 2.15e-04 (3.28e-04)	Tok/s 80495 (85762)	Loss/tok 2.8545 (3.1557)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.186 (0.166)	Data 1.38e-04 (3.27e-04)	Tok/s 90065 (85757)	Loss/tok 3.0961 (3.1554)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.075 (0.166)	Data 2.01e-04 (3.26e-04)	Tok/s 71162 (85758)	Loss/tok 2.5930 (3.1550)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.183 (0.166)	Data 1.69e-04 (3.25e-04)	Tok/s 91305 (85763)	Loss/tok 3.1594 (3.1547)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.128 (0.165)	Data 2.56e-04 (3.25e-04)	Tok/s 80516 (85751)	Loss/tok 2.9873 (3.1541)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.129 (0.165)	Data 1.68e-04 (3.24e-04)	Tok/s 78641 (85759)	Loss/tok 2.8718 (3.1534)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.129 (0.165)	Data 1.69e-04 (3.23e-04)	Tok/s 80049 (85749)	Loss/tok 2.8113 (3.1529)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.311 (0.165)	Data 1.46e-04 (3.22e-04)	Tok/s 96475 (85733)	Loss/tok 3.3983 (3.1523)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.185 (0.165)	Data 1.68e-04 (3.21e-04)	Tok/s 90511 (85731)	Loss/tok 3.0331 (3.1519)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.187 (0.165)	Data 1.66e-04 (3.20e-04)	Tok/s 90071 (85748)	Loss/tok 3.0934 (3.1518)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.126 (0.165)	Data 2.17e-04 (3.19e-04)	Tok/s 80727 (85738)	Loss/tok 2.9965 (3.1511)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.183 (0.165)	Data 1.80e-04 (3.18e-04)	Tok/s 91195 (85730)	Loss/tok 3.2104 (3.1514)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.313 (0.165)	Data 1.30e-04 (3.18e-04)	Tok/s 94767 (85740)	Loss/tok 3.2881 (3.1513)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.243 (0.165)	Data 1.46e-04 (3.16e-04)	Tok/s 96675 (85735)	Loss/tok 3.2068 (3.1519)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.243 (0.165)	Data 9.58e-05 (3.15e-04)	Tok/s 96830 (85748)	Loss/tok 3.1659 (3.1515)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.185 (0.165)	Data 2.57e-04 (3.14e-04)	Tok/s 90449 (85742)	Loss/tok 3.1250 (3.1508)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1770/1938]	Time 0.185 (0.165)	Data 1.07e-04 (3.14e-04)	Tok/s 90747 (85760)	Loss/tok 3.2029 (3.1512)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.183 (0.165)	Data 1.69e-04 (3.13e-04)	Tok/s 92585 (85755)	Loss/tok 3.0434 (3.1508)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.131 (0.165)	Data 1.72e-04 (3.12e-04)	Tok/s 79736 (85751)	Loss/tok 2.9625 (3.1505)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1800/1938]	Time 0.130 (0.165)	Data 1.41e-04 (3.11e-04)	Tok/s 79741 (85770)	Loss/tok 2.8447 (3.1506)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.074 (0.165)	Data 1.40e-04 (3.10e-04)	Tok/s 70810 (85760)	Loss/tok 2.5042 (3.1503)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.184 (0.165)	Data 1.67e-04 (3.09e-04)	Tok/s 91636 (85767)	Loss/tok 3.0490 (3.1500)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.130 (0.165)	Data 2.03e-04 (3.08e-04)	Tok/s 80206 (85765)	Loss/tok 2.9765 (3.1500)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.125 (0.166)	Data 1.24e-04 (3.08e-04)	Tok/s 82599 (85790)	Loss/tok 2.8676 (3.1499)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.309 (0.166)	Data 2.68e-04 (3.07e-04)	Tok/s 97957 (85782)	Loss/tok 3.3734 (3.1497)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.184 (0.166)	Data 1.10e-04 (3.06e-04)	Tok/s 91979 (85805)	Loss/tok 3.0761 (3.1500)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.245 (0.166)	Data 1.41e-04 (3.05e-04)	Tok/s 94286 (85813)	Loss/tok 3.2842 (3.1508)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.073 (0.166)	Data 2.35e-04 (3.05e-04)	Tok/s 71528 (85797)	Loss/tok 2.6109 (3.1503)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.189 (0.166)	Data 1.69e-04 (3.04e-04)	Tok/s 89285 (85809)	Loss/tok 3.1007 (3.1505)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.190 (0.166)	Data 1.40e-04 (3.03e-04)	Tok/s 88728 (85841)	Loss/tok 3.1811 (3.1507)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.131 (0.166)	Data 1.74e-04 (3.02e-04)	Tok/s 79644 (85839)	Loss/tok 2.8368 (3.1503)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.183 (0.166)	Data 1.52e-04 (3.01e-04)	Tok/s 92866 (85827)	Loss/tok 3.0979 (3.1501)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.128 (0.166)	Data 1.42e-04 (3.01e-04)	Tok/s 80469 (85809)	Loss/tok 2.9592 (3.1496)	LR 5.000e-04
:::MLL 1571749004.065 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1571749004.066 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.621 (0.621)	Decoder iters 103.0 (103.0)	Tok/s 26578 (26578)
0: Running moses detokenizer
0: BLEU(score=24.06793113983801, counts=[37274, 18655, 10639, 6330], totals=[65716, 62713, 59710, 56712], precisions=[56.71982470022521, 29.746623507087843, 17.817785965499915, 11.161658908167583], bp=1.0, sys_len=65716, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1571749005.952 eval_accuracy: {"value": 24.07, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1571749005.953 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1486	Test BLEU: 24.07
0: Performance: Epoch: 3	Training: 686277 Tok/s
0: Finished epoch 3
:::MLL 1571749005.953 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1571749005.954 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-10-22 12:56:53 PM
RESULT,RNN_TRANSLATOR,,1328,nvidia,2019-10-22 12:34:45 PM
